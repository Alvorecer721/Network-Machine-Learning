{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deschena/networks_ml_epfl/blob/main/assignment4/Assignment4NML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMlo30vQDnic"
      },
      "source": [
        "# Assignment 4: Graph Neural Networks\n",
        "\n",
        "Contact: [Abdellah RAHMANI](mailto:abdellah.rahmani@epfl.ch) [Jinhan LIU](mailto:jinhan.liu@epfl.ch)\n",
        "\n",
        "## Rules\n",
        "\n",
        "> ⚠️ **Read carefully before starting**\n",
        "\n",
        "**Deadline:** May 16th\n",
        "\n",
        "**Grading:**\n",
        "* The integrality of Assignment 4 will be scaled to 100% and will amount to 25% of the overall assignments score.\n",
        "* The total number of points is **40**, the points for each exercise are stated in the instructions.\n",
        "* All team members will receive the same grade based on the team solution.\n",
        "* Collaboration between team members is encouraged. No collaboration between teams is allowed.\n",
        "\n",
        "**Expected output:**\n",
        "\n",
        "You will have coding and theoretical questions. Coding exercises shall be solved within the specified space:\n",
        "```python\n",
        "# Your solution here ###########################################################\n",
        "...\n",
        "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "```\n",
        "Anything outside shall not be touched, except if otherwise stated.\n",
        "\n",
        "Theoretical questions shall be answered in the following markdown cell. The first line will be \n",
        "> **Your answer here:**\n",
        "\n",
        "**Submission:**\n",
        "* Your submission is self-contained in the `.ipynb` file.\n",
        "\n",
        "* Code has to be clean and readable. Provide meaningful variable names and comment where needed.\n",
        "\n",
        "* Textual answers in [markdown cells][md_cells] shall be short: one to two\n",
        "  sentences. Math shall be written in [LaTeX][md_latex].\n",
        "    **NOTE**: handwritten notes pasted in the notebook are ignored\n",
        "\n",
        "* You cannot import any other library than we imported.\n",
        "\n",
        "* Make sure all cells are executed before submitting. I.e., if you open the notebook again it should show numerical results and plots. Cells not run are ignored.\n",
        "\n",
        "* Execute your notebook from a blank state before submission, to make sure it is reproducible. You can click \"Kernel\" then \"Restart Kernel and Run All Cells\" in Jupyter. We might re-run cells to ensure that the code is working and corresponds to the results.\n",
        "\n",
        "[md_cells]: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
        "[md_latex]: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html#LaTeX-equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITjEp1iN8wds"
      },
      "source": [
        "# Objective\n",
        "\n",
        "The purpose of this assignment is to first understand and explore the building blocks of Graph Neural Networks (GNN), and then try to implement more up-to-date GNN architectures or build up new ones using your own ideas. \n",
        "\n",
        "Through these two sections, we hope that you could obtain solid understanding of GNN components and architectures, and gain more experience in solving practical tasks using basic models and then try to improve upon it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACMrQA2rraO2"
      },
      "source": [
        "# First part: Understanding Pooling and Graph Convolutional Layers (20 pts)\n",
        "In the first part, you will try to go through our implementation of a Graph Convolution Network, and discover both the key components and the entire structure of a typical GNN. \n",
        "\n",
        "In this section, implementation is not required. Rather, you are requested to execute the various code sections, analyze their outcomes, and provide an explanation of the results. The objective is to understand pooling in graphs and also graph convolutional with spectral filtering. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAN8ZbiRIM9C"
      },
      "source": [
        "# Graph ConvNets in PyTorch\n",
        "\n",
        "PyTorch implementation of the NeurIPS'16 paper:\n",
        "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\n",
        "M Defferrard, X Bresson, P Vandergheynst\n",
        "Advances in Neural Information Processing Systems, 3844-3852, 2016\n",
        "[ArXiv preprint](https://arxiv.org/abs/1606.09375)\n",
        "\n",
        "## Code:\n",
        "\n",
        "The code provides a simple example of graph ConvNets for the MNIST classification task.\n",
        "The graph is a 8-nearest neighbor graph of a 2D grid.\n",
        "The signals on graph are the MNIST images vectorized as $28^2 \\times 1$ vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R84-PMawIM9H",
        "outputId": "cce313f7-459e-4701-9355-34304f93b4cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda available\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import collections\n",
        "import time\n",
        "import numpy as np\n",
        "import scipy\n",
        "from functools import partial\n",
        "import os\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('cuda available')\n",
        "    dtypeFloat = torch.cuda.FloatTensor\n",
        "    dtypeLong = torch.cuda.LongTensor\n",
        "    torch.cuda.manual_seed(1)\n",
        "else:\n",
        "    print('cuda not available')\n",
        "    dtypeFloat = torch.FloatTensor\n",
        "    dtypeLong = torch.LongTensor\n",
        "    torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3V0ifgWIM9M"
      },
      "source": [
        "# Connect to your drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-MTr6dtQ6Ih",
        "outputId": "dabf136a-6bfc-432f-b0ea-a41ffc246942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTeBzQHpZNjV",
        "outputId": "4fb6c6c0-71bc-4789-c948-2bca6d57f428"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['grid_graph.py', 'coarsening.py']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "ROOT_PATH = \"/content/drive/MyDrive/EPFL/Network ML/assignment4\"\n",
        "print(os.listdir(ROOT_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_fZIFwYZvxc"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(ROOT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-QfoaerIM9M"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "This code is only to load the Mnist data set, you just need to run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU9R11odIM9M"
      },
      "outputs": [],
      "source": [
        "def check_mnist_dataset_exists(path_data='./'):\n",
        "    flag_train_data = os.path.isfile(path_data + 'mnist/train_data.pt') \n",
        "    flag_train_label = os.path.isfile(path_data + 'mnist/train_label.pt') \n",
        "    flag_test_data = os.path.isfile(path_data + 'mnist/test_data.pt') \n",
        "    flag_test_label = os.path.isfile(path_data + 'mnist/test_label.pt') \n",
        "    if flag_train_data==False or flag_train_label==False or flag_test_data==False or flag_test_label==False:\n",
        "        print('MNIST dataset preprocessing...')\n",
        "        import torchvision\n",
        "        import torchvision.transforms as transforms\n",
        "        trainset = torchvision.datasets.MNIST(root=path_data + 'mnist/temp', train=True,\n",
        "                                                download=True, transform=transforms.ToTensor())\n",
        "        testset = torchvision.datasets.MNIST(root=path_data + 'mnist/temp', train=False,\n",
        "                                               download=True, transform=transforms.ToTensor())\n",
        "        train_data=torch.Tensor(60000,28,28)\n",
        "        train_label=torch.LongTensor(60000)\n",
        "        for idx , example in enumerate(trainset):\n",
        "            train_data[idx]=example[0].squeeze()\n",
        "            train_label[idx]=example[1]\n",
        "        torch.save(train_data,path_data + 'mnist/train_data.pt')\n",
        "        torch.save(train_label,path_data + 'mnist/train_label.pt')\n",
        "        test_data=torch.Tensor(10000,28,28)\n",
        "        test_label=torch.LongTensor(10000)\n",
        "        for idx , example in enumerate(testset):\n",
        "            test_data[idx]=example[0].squeeze()\n",
        "            test_label[idx]=example[1]\n",
        "        torch.save(test_data,path_data + 'mnist/test_data.pt')\n",
        "        torch.save(test_label,path_data + 'mnist/test_label.pt')\n",
        "    return path_data\n",
        "\n",
        "\n",
        "_ = check_mnist_dataset_exists()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-FvItWZIM9N",
        "outputId": "26ffcad3-ca7c-4b82-b826-fd1200a19d50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(60000,)\n",
            "(10000, 784)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "#if you want to play with a small dataset (for cpu), uncomment.\n",
        "nb_selected_train_data = 60000\n",
        "nb_selected_test_data = 10000\n",
        "\n",
        "train_data=torch.load('mnist/train_data.pt').reshape(60000,784).numpy()\n",
        "train_data = train_data[:nb_selected_train_data,:]\n",
        "print(train_data.shape)\n",
        "\n",
        "train_labels=torch.load('mnist/train_label.pt').numpy()\n",
        "train_labels = train_labels[:nb_selected_train_data]\n",
        "print(train_labels.shape)\n",
        "\n",
        "test_data=torch.load('mnist/test_data.pt').reshape(10000,784).numpy()\n",
        "test_data = test_data[:nb_selected_test_data,:]\n",
        "print(test_data.shape)\n",
        "\n",
        "test_labels=torch.load('mnist/test_label.pt').numpy()\n",
        "test_labels = test_labels[:nb_selected_test_data]\n",
        "print(test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u_wcW0U8AFF"
      },
      "source": [
        "Here we construct the graph using the 8-nearest neighbors of a 2D grid using Euclidean distance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4d8RsqlIM9O",
        "outputId": "0336ca49-843d-44cf-dcdd-38ccef7c8a8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nb edges:  6396\n"
          ]
        }
      ],
      "source": [
        "from grid_graph import grid_graph\n",
        "from coarsening import coarsen, HEM, compute_perm, perm_adjacency\n",
        "from coarsening import perm_data\n",
        "\n",
        "# Construct graph\n",
        "t_start = time.time()\n",
        "grid_side = 28\n",
        "number_edges = 8\n",
        "metric = 'euclidean'\n",
        "\n",
        "\n",
        "######## YOUR GRAPH ADJACENCY MATRIX HERE ########\n",
        "A = grid_graph(grid_side,number_edges,metric) # create graph of Euclidean grid\n",
        "######## YOUR GRAPH ADJACENCY MATRIX HERE ########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C7k8PTnFAh2"
      },
      "source": [
        "In the following two cells we build up functions for calculating the graph Laplacian and retrieving the largest eigenvalue of it with scaling ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1bL7L1JIM9P"
      },
      "outputs": [],
      "source": [
        "def laplacian(W, normalized=True):\n",
        "    \"\"\"Return graph Laplacian\"\"\"\n",
        "    I = scipy.sparse.identity(W.shape[0], dtype=W.dtype)\n",
        "\n",
        "    #W += I\n",
        "    # Degree matrix.\n",
        "    d = W.sum(axis=0)\n",
        "    # Laplacian matrix.\n",
        "    if not normalized:\n",
        "        D = scipy.sparse.diags(d.A.squeeze(), 0)\n",
        "        L = D - W\n",
        "    else:\n",
        "        d_inv = 1/d\n",
        "        Dinv = scipy.sparse.csr_matrix.sqrt(scipy.sparse.diags(d_inv.A.squeeze(), 0))\n",
        "        L =  - Dinv.dot(W.dot(Dinv))# i modified this normalization in order to use it in question 5 Lsym  = Lsym-I\n",
        "\n",
        "        pass\n",
        "\n",
        "    assert np.abs(L - L.T).mean() < 1e-8\n",
        "    assert type(L) is scipy.sparse.csr.csr_matrix\n",
        "    return L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PsIUNxsIM9P"
      },
      "outputs": [],
      "source": [
        "def rescale_L(L, lmax=2):\n",
        "    \"\"\"Rescale Laplacian eigenvalues to [-1,1]\"\"\"\n",
        "    M, M = L.shape\n",
        "    I = scipy.sparse.identity(M, format='csr', dtype=L.dtype)\n",
        "    L /= lmax * 2\n",
        "    L -= I\n",
        "    return L \n",
        "\n",
        "def lmax_L(L):\n",
        "    \"\"\"Compute largest Laplacian eigenvalue\"\"\"\n",
        "    return scipy.sparse.linalg.eigsh(L, k=1, which='LM', return_eigenvectors=False)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOYcC1FIFe6V"
      },
      "source": [
        "In the following cells, we can try to implement the graph coarsening with a specific coarsening level and retrieve the corresponding largest eigenvalues of Laplacians of the coarsened graphs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRkKisnu29dK",
        "outputId": "fc6feb55-c6d7-4541-93e9-45770c617906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Heavy Edge Matching coarsening with Xavier version\n",
            "Layer 0: M_0 = |V| = 960 nodes (176 added), |E| = 3198 edges\n",
            "Layer 1: M_1 = |V| = 480 nodes (77 added), |E| = 1618 edges\n",
            "Layer 2: M_2 = |V| = 240 nodes (29 added), |E| = 781 edges\n",
            "Layer 3: M_3 = |V| = 120 nodes (7 added), |E| = 388 edges\n",
            "Layer 4: M_4 = |V| = 60 nodes (0 added), |E| = 194 edges\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-d29d56694506>:13: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  d_inv = 1/d\n",
            "<ipython-input-9-d29d56694506>:20: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
            "  assert type(L) is scipy.sparse.csr.csr_matrix\n"
          ]
        }
      ],
      "source": [
        "# Compute coarsened graphs\n",
        "coarsening_levels = 4\n",
        "\n",
        "L, perm = coarsen(A, coarsening_levels, partial(laplacian, normalized=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSy9EM2KIM9P",
        "outputId": "cd2dfe50-b450-4866-f784-9d7d8ae9fd46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Heavy Edge Matching coarsening with Xavier version\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-d29d56694506>:20: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
            "  assert type(L) is scipy.sparse.csr.csr_matrix\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 0: M_0 = |V| = 960 nodes (176 added), |E| = 3198 edges\n",
            "Layer 1: M_1 = |V| = 480 nodes (77 added), |E| = 1618 edges\n",
            "Layer 2: M_2 = |V| = 240 nodes (29 added), |E| = 781 edges\n",
            "Layer 3: M_3 = |V| = 120 nodes (7 added), |E| = 388 edges\n",
            "Layer 4: M_4 = |V| = 60 nodes (0 added), |E| = 194 edges\n",
            "lmax: [5.87704, 11.296647, 19.596706, 30.225954]\n",
            "Execution time: 1.72s\n"
          ]
        }
      ],
      "source": [
        "# Compute coarsened graphs\n",
        "coarsening_levels = 4\n",
        "\n",
        "L, perm = coarsen(A, coarsening_levels, partial(laplacian, normalized=False))\n",
        "\n",
        "# Compute max eigenvalue of graph Laplacians\n",
        "lmax = []\n",
        "for i in range(coarsening_levels):\n",
        "    lmax.append(lmax_L(L[i]))\n",
        "print('lmax: ' + str([lmax[i] for i in range(coarsening_levels)]))\n",
        "\n",
        "# Reindex nodes to satisfy a binary tree structure\n",
        "train_data = perm_data(train_data, perm)\n",
        "test_data = perm_data(test_data, perm)\n",
        "\n",
        "print('Execution time: {:.2f}s'.format(time.time() - t_start))\n",
        "del perm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc0SwCcgIM9R"
      },
      "source": [
        "Here, we implemented the pooling layers and computed the list `L` containing the Laplacians of the graphs for each layer.\n",
        "\n",
        "## <font color='red'>Question 1: what is the size of the various poolings? (2pts) </font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPsKBfCJIiOX"
      },
      "source": [
        "> **Your answer here:** The size of the various pooling is 944, 472, 236, 118, 59. These are obtained after each round of the Heavy Edge Matching algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY4IypYiIM9R"
      },
      "source": [
        "# Graph ConvNet LeNet5\n",
        "\n",
        "With the components including graph coarsening and graph Laplacian computation, now we can build a entire GNN architecture for the image classification task, such as the classic Graph ConvNet LeNet5, which contains 2 graph convolutional layers, 2 pooling layers with size 4 and two fully-connected layers as shown below:\n",
        "\n",
        "1. Convolutional layer (Dim=32)\n",
        "2. Max pooling layer (Dim=4)\n",
        "3. Convolutional layer (Dim=64)\n",
        "4. Max pooling layer (Dim=4)\n",
        "5. Fully-connected layer (Dim=512)\n",
        "6. Fully-connected layer (Dim=10)\n",
        "\n",
        "## <font color='red'>Question 2: which graphs will you take in the list `L` for the graph convolutional layers? (2pts)</font> \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTMYxUq7JDhx"
      },
      "source": [
        "> **Your answer here:** The graphs 0 and 2, i.e. the ones with 944 and 236 nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pndzzP33LBua"
      },
      "source": [
        "Let's define a torch class of GNN module and initialize our Graph ConvNet LeNet5 using its network parameter settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKiPIVdKIM9R"
      },
      "outputs": [],
      "source": [
        "class Graph_ConvNet_LeNet5(nn.Module):\n",
        "    \n",
        "    def __init__(self, net_parameters, rescale = True):\n",
        "        \n",
        "        print('Graph ConvNet: LeNet5')\n",
        "        \n",
        "        super(Graph_ConvNet_LeNet5, self).__init__()\n",
        "        \n",
        "        # parameters\n",
        "        self.rescale = rescale\n",
        "        D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F = net_parameters\n",
        "        FC1Fin = CL2_F*(D//16)\n",
        "        \n",
        "        # graph CL1\n",
        "        self.cl1 = nn.Linear(CL1_K, CL1_F) \n",
        "        self.init_layers(self.cl1, CL1_K, CL1_F)\n",
        "        self.CL1_K = CL1_K; self.CL1_F = CL1_F; \n",
        "        \n",
        "        # graph CL2\n",
        "        self.cl2 = nn.Linear(CL2_K*CL1_F, CL2_F) \n",
        "        self.init_layers(self.cl2, CL2_K*CL1_F, CL2_F)\n",
        "        self.CL2_K = CL2_K; self.CL2_F = CL2_F; \n",
        "\n",
        "        # FC1\n",
        "        self.fc1 = nn.Linear(FC1Fin, FC1_F) \n",
        "        self.init_layers(self.fc1, FC1Fin, FC1_F)\n",
        "        self.FC1Fin = FC1Fin\n",
        "        \n",
        "        # FC2\n",
        "        self.fc2 = nn.Linear(FC1_F, FC2_F)\n",
        "        self.init_layers(self.fc2, FC1_F, FC2_F)\n",
        "\n",
        "        # nb of parameters\n",
        "        nb_param = CL1_K* CL1_F + CL1_F          # CL1\n",
        "        nb_param += CL2_K* CL1_F* CL2_F + CL2_F  # CL2\n",
        "        nb_param += FC1Fin* FC1_F + FC1_F        # FC1\n",
        "        nb_param += FC1_F* FC2_F + FC2_F         # FC2\n",
        "        print('nb of parameters=',nb_param,'\\n')\n",
        "        \n",
        "        \n",
        "    def init_layers(self, W, Fin, Fout):\n",
        "\n",
        "        scale = np.sqrt( 2.0/ (Fin+Fout) )\n",
        "        W.weight.data.uniform_(-scale, scale)\n",
        "        W.bias.data.fill_(0.0)\n",
        "\n",
        "        return W\n",
        "        \n",
        "        \n",
        "    def graph_conv_cheby(self, x, cl, L, lmax, Fout, K):\n",
        "        # parameters\n",
        "        # B = batch size\n",
        "        # V = nb vertices\n",
        "        # Fin = nb input features\n",
        "        # Fout = nb output features\n",
        "        # K = Chebyshev order & support size\n",
        "        B, V, Fin = x.size(); B, V, Fin = int(B), int(V), int(Fin) \n",
        "\n",
        "        # rescale Laplacian\n",
        "        lmax = lmax_L(L)\n",
        "        if self.rescale:\n",
        "          \n",
        "          L = rescale_L(L, lmax) \n",
        "        \n",
        "        # convert scipy sparse matric L to pytorch\n",
        "        L = L.tocoo()\n",
        "        indices = np.column_stack((L.row, L.col)).T \n",
        "        indices = indices.astype(np.int64)\n",
        "        indices = torch.from_numpy(indices)\n",
        "        indices = indices.type(torch.LongTensor)\n",
        "        L_data = L.data.astype(np.float32)\n",
        "        L_data = torch.from_numpy(L_data) \n",
        "        L_data = L_data.type(torch.FloatTensor)\n",
        "        L = torch.sparse.FloatTensor(indices, L_data, torch.Size(L.shape))\n",
        "        L.requires_grad_(False)\n",
        "        if torch.cuda.is_available():\n",
        "            L = L.cuda()\n",
        "        # transform to Chebyshev basis\n",
        "        x0 = x.permute(1,2,0).contiguous()  # V x Fin x B\n",
        "        x0 = x0.view([V, Fin*B])            # V x Fin*B\n",
        "        x = x0.unsqueeze(0)                 # 1 x V x Fin*B\n",
        "        \n",
        "        def concat(x, x_):\n",
        "            x_ = x_.unsqueeze(0)            # 1 x V x Fin*B\n",
        "            return torch.cat((x, x_), 0)    # K x V x Fin*B  \n",
        "             \n",
        "        if K > 1: \n",
        "            x1 = torch.mm(L,x0)              # V x Fin*B\n",
        "            x = torch.cat((x, x1.unsqueeze(0)),0)  # 2 x V x Fin*B\n",
        "        for k in range(2, K):\n",
        "            x2 = 2 * torch.mm(L,x1) - x0  \n",
        "            x = torch.cat((x, x2.unsqueeze(0)),0)  # M x Fin*B\n",
        "            x0, x1 = x1, x2  \n",
        "        \n",
        "        x = x.view([K, V, Fin, B])           # K x V x Fin x B     \n",
        "        x = x.permute(3,1,2,0).contiguous()  # B x V x Fin x K       \n",
        "        x = x.view([B*V, Fin*K])             # B*V x Fin*K\n",
        "        \n",
        "        # Compose linearly Fin features to get Fout features\n",
        "        x = cl(x)                            # B*V x Fout  \n",
        "        x = x.view([B, V, Fout])             # B x V x Fout\n",
        "        return x\n",
        "        \n",
        "        \n",
        "    # Max pooling of size p. Must be a power of 2.\n",
        "    def graph_max_pool(self, x, p): \n",
        "        if p > 1: \n",
        "            x = x.permute(0,2,1).contiguous()  # x = B x F x V\n",
        "            x = nn.MaxPool1d(p)(x)             # B x F x V/p          \n",
        "            x = x.permute(0,2,1).contiguous()  # x = B x V/p x F\n",
        "            return x  \n",
        "        else:\n",
        "            return x   \n",
        "        \n",
        "        \n",
        "    def forward(self, x, d, L, lmax):\n",
        "        # graph CL1\n",
        "        x = x.unsqueeze(2) # B x V x Fin=1  \n",
        "        x = self.graph_conv_cheby(x, self.cl1, L[0], lmax[0], self.CL1_F, self.CL1_K)\n",
        "        x = F.relu(x)\n",
        "        x = self.graph_max_pool(x, 4)\n",
        "        # graph CL2\n",
        "        x = self.graph_conv_cheby(x, self.cl2, L[2], lmax[2], self.CL2_F, self.CL2_K)\n",
        "        x = F.relu(x)\n",
        "        x = self.graph_max_pool(x, 4)\n",
        "        # FC1\n",
        "        x = x.view(-1, self.FC1Fin)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x  = nn.Dropout(d)(x)\n",
        "        # FC2\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "        \n",
        "        \n",
        "    def loss(self, y, y_target, l2_regularization):\n",
        "    \n",
        "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
        "\n",
        "        l2_loss = 0.0\n",
        "        for param in self.parameters():\n",
        "            data = param* param\n",
        "            l2_loss += data.sum()\n",
        "           \n",
        "        loss += 0.5* l2_regularization* l2_loss\n",
        "            \n",
        "        return loss\n",
        "    \n",
        "    \n",
        "    def update(self, lr):\n",
        "                \n",
        "        update = torch.optim.SGD( self.parameters(), lr=lr, momentum=0.9 )\n",
        "        \n",
        "        return update\n",
        "        \n",
        "        \n",
        "    def update_learning_rate(self, optimizer, lr):\n",
        "   \n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    \n",
        "    def evaluation(self, y_predicted, test_l):\n",
        "    \n",
        "        _, class_predicted = torch.max(y_predicted.data, 1)\n",
        "        return 100.0* (class_predicted == test_l).sum()/ y_predicted.size(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzp6aP9YIM9T",
        "outputId": "f216da06-c1f4-423e-dec9-3bee6fd138ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No existing network to delete\n",
            "\n",
            "Graph ConvNet: LeNet5\n",
            "nb of parameters= 2023818 \n",
            "\n",
            "Graph_ConvNet_LeNet5(\n",
            "  (cl1): Linear(in_features=25, out_features=32, bias=True)\n",
            "  (cl2): Linear(in_features=800, out_features=64, bias=True)\n",
            "  (fc1): Linear(in_features=3840, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Delete existing network if exists\n",
        "try:\n",
        "    del net\n",
        "    print('Delete existing network\\n')\n",
        "except NameError:\n",
        "    print('No existing network to delete\\n')\n",
        "\n",
        "# network parameters\n",
        "D = train_data.shape[1]\n",
        "CL1_F = 32\n",
        "CL1_K = 25\n",
        "CL2_F = 64\n",
        "CL2_K = 25\n",
        "FC1_F = 512\n",
        "FC2_F = 10\n",
        "net_parameters = [D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F]\n",
        "dropout_value = 0.5\n",
        "\n",
        "# instantiate the object net of the class \n",
        "net = Graph_ConvNet_LeNet5(net_parameters)\n",
        "if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAHvOqwFIM9T"
      },
      "source": [
        "Good time, to check the network is working. Now we can partition the dataset and evaluate our GNN upon it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUBExiK7IM9U",
        "outputId": "4a47ca6a-d2b9-460e-be1c-f00022767c5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([5, 10])\n"
          ]
        }
      ],
      "source": [
        "train_x, train_y = train_data[:5,:], train_labels[:5]\n",
        "train_x =  torch.FloatTensor(train_x).type(dtypeFloat)\n",
        "train_y = train_y.astype(np.int64)\n",
        "train_y = torch.LongTensor(train_y).type(dtypeLong) \n",
        "            \n",
        "# Forward \n",
        "y = net(train_x, dropout_value, L, lmax)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pqSKZucIM9U",
        "outputId": "21961d12-8ad6-44ae-ecc3-b55ca19f131a",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_epochs= 3 , train_size= 60000 , nb_iter= 1800\n",
            "epoch= 1, i=  100, loss(batch)= 0.3013, accuray(batch)= 91.00\n",
            "epoch= 1, i=  200, loss(batch)= 0.3002, accuray(batch)= 97.00\n",
            "epoch= 1, i=  300, loss(batch)= 0.2580, accuray(batch)= 96.00\n",
            "epoch= 1, i=  400, loss(batch)= 0.2740, accuray(batch)= 95.00\n",
            "epoch= 1, i=  500, loss(batch)= 0.2084, accuray(batch)= 96.00\n",
            "epoch= 1, i=  600, loss(batch)= 0.1553, accuray(batch)= 98.00\n",
            "epoch= 1, loss(train)= 0.375, accuracy(train)= 91.413, time= 86.526, lr= 0.05000\n",
            "  accuracy(test) = 97.860 %, time= 9.610\n",
            "epoch= 2, i=  100, loss(batch)= 0.1469, accuray(batch)= 98.00\n",
            "epoch= 2, i=  200, loss(batch)= 0.1773, accuray(batch)= 98.00\n",
            "epoch= 2, i=  300, loss(batch)= 0.1448, accuray(batch)= 98.00\n",
            "epoch= 2, i=  400, loss(batch)= 0.1672, accuray(batch)= 97.00\n",
            "epoch= 2, i=  500, loss(batch)= 0.1747, accuray(batch)= 98.00\n",
            "epoch= 2, i=  600, loss(batch)= 0.1963, accuray(batch)= 96.00\n",
            "epoch= 2, loss(train)= 0.175, accuracy(train)= 97.840, time= 75.629, lr= 0.04750\n",
            "  accuracy(test) = 98.660 %, time= 9.328\n",
            "epoch= 3, i=  100, loss(batch)= 0.1542, accuray(batch)= 98.00\n",
            "epoch= 3, i=  200, loss(batch)= 0.2339, accuray(batch)= 97.00\n",
            "epoch= 3, i=  300, loss(batch)= 0.1689, accuray(batch)= 97.00\n",
            "epoch= 3, i=  400, loss(batch)= 0.1227, accuray(batch)= 100.00\n",
            "epoch= 3, i=  500, loss(batch)= 0.1245, accuray(batch)= 99.00\n",
            "epoch= 3, i=  600, loss(batch)= 0.1108, accuray(batch)= 99.00\n",
            "epoch= 3, loss(train)= 0.148, accuracy(train)= 98.358, time= 68.324, lr= 0.04512\n",
            "  accuracy(test) = 98.720 %, time= 9.368\n"
          ]
        }
      ],
      "source": [
        "# Weights\n",
        "L_net = list(net.parameters())\n",
        "\n",
        "# learning parameters\n",
        "learning_rate = 0.05\n",
        "l2_regularization = 5e-4 \n",
        "batch_size = 100\n",
        "num_epochs = 3\n",
        "train_size = train_data.shape[0]\n",
        "nb_iter = int(num_epochs * train_size) // batch_size\n",
        "print('num_epochs=',num_epochs,', train_size=',train_size,', nb_iter=',nb_iter)\n",
        "\n",
        "# Optimizer\n",
        "global_lr = learning_rate\n",
        "global_step = 0\n",
        "decay = 0.95\n",
        "decay_steps = train_size\n",
        "lr = learning_rate\n",
        "optimizer = net.update(lr) \n",
        "\n",
        "# loop over epochs\n",
        "indices = collections.deque()\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    # reshuffle \n",
        "    indices.extend(np.random.permutation(train_size)) # rand permutation\n",
        "    \n",
        "    # reset time\n",
        "    t_start = time.time()\n",
        "    \n",
        "    # extract batches\n",
        "    running_loss = 0.0\n",
        "    running_accuray = 0\n",
        "    running_total = 0\n",
        "    while len(indices) >= batch_size:\n",
        "        \n",
        "        # extract batches\n",
        "        batch_idx = [indices.popleft() for i in range(batch_size)]\n",
        "        train_x, train_y = train_data[batch_idx,:], train_labels[batch_idx]\n",
        "        train_x =  torch.FloatTensor(train_x).type(dtypeFloat)\n",
        "        train_y = train_y.astype(np.int64)\n",
        "        train_y = torch.LongTensor(train_y).type(dtypeLong) \n",
        "            \n",
        "        # Forward \n",
        "        y = net(train_x, dropout_value, L, lmax)\n",
        "        loss = net.loss(y,train_y,l2_regularization) \n",
        "        loss_train = loss.detach().item()\n",
        "        # Accuracy\n",
        "        acc_train = net.evaluation(y,train_y.data)\n",
        "        # backward\n",
        "        loss.backward()\n",
        "        # Update \n",
        "        global_step += batch_size # to update learning rate\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # loss, accuracy\n",
        "        running_loss += loss_train\n",
        "        running_accuray += acc_train\n",
        "        running_total += 1\n",
        "        # print        \n",
        "        if not running_total%100: # print every x mini-batches\n",
        "            print('epoch= %d, i= %4d, loss(batch)= %.4f, accuray(batch)= %.2f' % (epoch+1, running_total, loss_train, acc_train))\n",
        "          \n",
        "    # print \n",
        "    t_stop = time.time() - t_start\n",
        "    print('epoch= %d, loss(train)= %.3f, accuracy(train)= %.3f, time= %.3f, lr= %.5f' % \n",
        "          (epoch+1, running_loss/running_total, running_accuray/running_total, t_stop, lr))\n",
        " \n",
        "    # update learning rate \n",
        "    lr = global_lr * pow( decay , float(global_step// decay_steps) )\n",
        "    optimizer = net.update_learning_rate(optimizer, lr)\n",
        "    \n",
        "    \n",
        "    # Test set\n",
        "    with torch.no_grad():\n",
        "        running_accuray_test = 0\n",
        "        running_total_test = 0\n",
        "        indices_test = collections.deque()\n",
        "        indices_test.extend(range(test_data.shape[0]))\n",
        "        t_start_test = time.time()\n",
        "        while len(indices_test) >= batch_size:\n",
        "            batch_idx_test = [indices_test.popleft() for i in range(batch_size)]\n",
        "            test_x, test_y = test_data[batch_idx_test,:], test_labels[batch_idx_test]\n",
        "            test_x = torch.FloatTensor(test_x).type(dtypeFloat)\n",
        "            y = net(test_x, 0.0, L, lmax) \n",
        "            test_y = test_y.astype(np.int64)\n",
        "            test_y = torch.LongTensor(test_y).type(dtypeLong)\n",
        "            acc_test = net.evaluation(y,test_y.data)\n",
        "            running_accuray_test += acc_test\n",
        "            running_total_test += 1\n",
        "        t_stop_test = time.time() - t_start_test\n",
        "        print('  accuracy(test) = %.3f %%, time= %.3f' % (running_accuray_test / running_total_test, t_stop_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcMxnNIiXLyc"
      },
      "source": [
        "### <font color='red'>Question 3: In this code, each convolutional layer has a parameter K. What does it represent? What are the consequences of choosing a higher or lower value of K? (4pts)</font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1sYCHHfXxEX"
      },
      "source": [
        "> **Your answer here:** The parameter $K$ is a hyperparameter of the polynomial kernel of the model. Instead of using non-parametric filters where the parameters associated with each frequency are independent, as they scale as $\\mathcal O(|\\mathcal V|)$, we parametrize the filter using Chebyshev polynomials so that filters are constrained in a $K$-hop neighborhood around vertices.\n",
        "\n",
        "\n",
        "> Hence, $K$ represents the size of the filter, which determines the size of the neighborhood that can affect a vertex. Choosing a higher value of $K$ means each vertex can access features of further nodes. It also increases the number of parameters of the model, it's ability to express complex functions but also increases the risk of overfitting. More precisely, a filter with $K$ means the filter can use node features of nodes at shortest-path distance up to (and including) to $K$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqHiWQ8iIM9U"
      },
      "source": [
        "### <font color='red'>Question 4: Is it necessary to rescale the Laplacian (in the function `rescale_L`)? (4pts) Try to remove it and explain what happens. </font> \n",
        "\n",
        "Hint: See Section 2.1 of [the paper](https://arxiv.org/pdf/1606.09375.pdf).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0l0xPYW0o5o",
        "outputId": "ae6f553f-b0fb-440c-e95b-aadc0f0e5ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Delete existing network\n",
            "\n",
            "Graph ConvNet: LeNet5\n",
            "nb of parameters= 2023818 \n",
            "\n",
            "Graph_ConvNet_LeNet5(\n",
            "  (cl1): Linear(in_features=25, out_features=32, bias=True)\n",
            "  (cl2): Linear(in_features=800, out_features=64, bias=True)\n",
            "  (fc1): Linear(in_features=3840, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Delete existing network if exists\n",
        "try:\n",
        "    del net\n",
        "    print('Delete existing network\\n')\n",
        "except NameError:\n",
        "    print('No existing network to delete\\n')\n",
        "\n",
        "# network parameters\n",
        "D = train_data.shape[1]\n",
        "CL1_F = 32\n",
        "CL1_K = 25\n",
        "CL2_F = 64\n",
        "CL2_K = 25\n",
        "FC1_F = 512\n",
        "FC2_F = 10\n",
        "net_parameters = [D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F]\n",
        "dropout_value = 0.5\n",
        "\n",
        "# instantiate the object net of the class, !!! we removed the rescale\n",
        "net = Graph_ConvNet_LeNet5(net_parameters, rescale = False)\n",
        "if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4KXZMF0CHdz",
        "outputId": "e996fb38-3d01-4541-f23f-a5be6eadcaec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Heavy Edge Matching coarsening with Xavier version\n",
            "Layer 0: M_0 = |V| = 960 nodes (176 added), |E| = 3198 edges\n",
            "Layer 1: M_1 = |V| = 480 nodes (77 added), |E| = 1618 edges\n",
            "Layer 2: M_2 = |V| = 240 nodes (29 added), |E| = 781 edges\n",
            "Layer 3: M_3 = |V| = 120 nodes (7 added), |E| = 388 edges\n",
            "Layer 4: M_4 = |V| = 60 nodes (0 added), |E| = 194 edges\n",
            "lmax: [5.8770437, 11.296648, 19.596704, 30.225956]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-d29d56694506>:20: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
            "  assert type(L) is scipy.sparse.csr.csr_matrix\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution time: 79.04s\n"
          ]
        }
      ],
      "source": [
        "# Compute coarsened graphs\n",
        "coarsening_levels = 4\n",
        "\n",
        "L, perm = coarsen(A, coarsening_levels, partial(laplacian, normalized=False))\n",
        "\n",
        "# Compute max eigenvalue of graph Laplacians\n",
        "lmax = []\n",
        "for i in range(coarsening_levels):\n",
        "    lmax.append(lmax_L(L[i]))\n",
        "print('lmax: ' + str([lmax[i] for i in range(coarsening_levels)]))\n",
        "\n",
        "# Reindex nodes to satisfy a binary tree structure\n",
        "train_data = perm_data(train_data, perm)\n",
        "test_data = perm_data(test_data, perm)\n",
        "\n",
        "print('Execution time: {:.2f}s'.format(time.time() - t_start))\n",
        "del perm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L6jw1Nh0o_6",
        "outputId": "ee3634cb-9367-454d-b053-614ebeed6dc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_epochs= 3 , train_size= 60000 , nb_iter= 1800\n",
            "epoch= 1, i=  100, loss(batch)= nan, accuray(batch)= 5.00\n",
            "epoch= 1, i=  200, loss(batch)= nan, accuray(batch)= 6.00\n",
            "epoch= 1, i=  300, loss(batch)= nan, accuray(batch)= 11.00\n",
            "epoch= 1, i=  400, loss(batch)= nan, accuray(batch)= 17.00\n",
            "epoch= 1, i=  500, loss(batch)= nan, accuray(batch)= 11.00\n",
            "epoch= 1, i=  600, loss(batch)= nan, accuray(batch)= 10.00\n",
            "epoch= 1, loss(train)= nan, accuracy(train)= 9.872, time= 76.965, lr= 0.05000\n",
            "  accuracy(test) = 9.800 %, time= 6.616\n",
            "epoch= 2, i=  100, loss(batch)= nan, accuray(batch)= 10.00\n",
            "epoch= 2, i=  200, loss(batch)= nan, accuray(batch)= 7.00\n",
            "epoch= 2, i=  300, loss(batch)= nan, accuray(batch)= 3.00\n",
            "epoch= 2, i=  400, loss(batch)= nan, accuray(batch)= 15.00\n",
            "epoch= 2, i=  500, loss(batch)= nan, accuray(batch)= 9.00\n",
            "epoch= 2, i=  600, loss(batch)= nan, accuray(batch)= 6.00\n",
            "epoch= 2, loss(train)= nan, accuracy(train)= 9.872, time= 80.423, lr= 0.04750\n",
            "  accuracy(test) = 9.800 %, time= 8.344\n",
            "epoch= 3, i=  100, loss(batch)= nan, accuray(batch)= 9.00\n",
            "epoch= 3, i=  200, loss(batch)= nan, accuray(batch)= 8.00\n",
            "epoch= 3, i=  300, loss(batch)= nan, accuray(batch)= 9.00\n",
            "epoch= 3, i=  400, loss(batch)= nan, accuray(batch)= 9.00\n",
            "epoch= 3, i=  500, loss(batch)= nan, accuray(batch)= 8.00\n",
            "epoch= 3, i=  600, loss(batch)= nan, accuray(batch)= 6.00\n",
            "epoch= 3, loss(train)= nan, accuracy(train)= 9.872, time= 73.256, lr= 0.04512\n",
            "  accuracy(test) = 9.800 %, time= 6.656\n"
          ]
        }
      ],
      "source": [
        "# Weights\n",
        "L_net = list(net.parameters())\n",
        "\n",
        "# learning parameters\n",
        "learning_rate = 0.05\n",
        "l2_regularization = 5e-4 \n",
        "batch_size = 100\n",
        "num_epochs = 3\n",
        "train_size = train_data.shape[0]\n",
        "nb_iter = int(num_epochs * train_size) // batch_size\n",
        "print('num_epochs=',num_epochs,', train_size=',train_size,', nb_iter=',nb_iter)\n",
        "\n",
        "# Optimizer\n",
        "global_lr = learning_rate\n",
        "global_step = 0\n",
        "decay = 0.95\n",
        "decay_steps = train_size\n",
        "lr = learning_rate\n",
        "optimizer = net.update(lr) \n",
        "\n",
        "# loop over epochs\n",
        "indices = collections.deque()\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    # reshuffle \n",
        "    indices.extend(np.random.permutation(train_size)) # rand permutation\n",
        "    \n",
        "    # reset time\n",
        "    t_start = time.time()\n",
        "    \n",
        "    # extract batches\n",
        "    running_loss = 0.0\n",
        "    running_accuray = 0\n",
        "    running_total = 0\n",
        "    while len(indices) >= batch_size:\n",
        "        \n",
        "        # extract batches\n",
        "        batch_idx = [indices.popleft() for i in range(batch_size)]\n",
        "        train_x, train_y = train_data[batch_idx,:], train_labels[batch_idx]\n",
        "        train_x =  torch.FloatTensor(train_x).type(dtypeFloat)\n",
        "        train_y = train_y.astype(np.int64)\n",
        "        train_y = torch.LongTensor(train_y).type(dtypeLong) \n",
        "            \n",
        "        # Forward \n",
        "        y = net(train_x, dropout_value, L, lmax)\n",
        "        loss = net.loss(y,train_y,l2_regularization) \n",
        "        loss_train = loss.detach().item()\n",
        "        # Accuracy\n",
        "        acc_train = net.evaluation(y,train_y.data)\n",
        "        # backward\n",
        "        loss.backward()\n",
        "        # Update \n",
        "        global_step += batch_size # to update learning rate\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # loss, accuracy\n",
        "        running_loss += loss_train\n",
        "        running_accuray += acc_train\n",
        "        running_total += 1\n",
        "        # print        \n",
        "        if not running_total%100: # print every x mini-batches\n",
        "            print('epoch= %d, i= %4d, loss(batch)= %.4f, accuray(batch)= %.2f' % (epoch+1, running_total, loss_train, acc_train))\n",
        "          \n",
        "    # print \n",
        "    t_stop = time.time() - t_start\n",
        "    print('epoch= %d, loss(train)= %.3f, accuracy(train)= %.3f, time= %.3f, lr= %.5f' % \n",
        "          (epoch+1, running_loss/running_total, running_accuray/running_total, t_stop, lr))\n",
        " \n",
        "    # update learning rate \n",
        "    lr = global_lr * pow( decay , float(global_step// decay_steps) )\n",
        "    optimizer = net.update_learning_rate(optimizer, lr)\n",
        "    \n",
        "    \n",
        "    # Test set\n",
        "    with torch.no_grad():\n",
        "        running_accuray_test = 0\n",
        "        running_total_test = 0\n",
        "        indices_test = collections.deque()\n",
        "        indices_test.extend(range(test_data.shape[0]))\n",
        "        t_start_test = time.time()\n",
        "        while len(indices_test) >= batch_size:\n",
        "            batch_idx_test = [indices_test.popleft() for i in range(batch_size)]\n",
        "            test_x, test_y = test_data[batch_idx_test,:], test_labels[batch_idx_test]\n",
        "            test_x = torch.FloatTensor(test_x).type(dtypeFloat)\n",
        "            y = net(test_x, 0.0, L, lmax) \n",
        "            test_y = test_y.astype(np.int64)\n",
        "            test_y = torch.LongTensor(test_y).type(dtypeLong)\n",
        "            acc_test = net.evaluation(y,test_y.data)\n",
        "            running_accuray_test += acc_test\n",
        "            running_total_test += 1\n",
        "        t_stop_test = time.time() - t_start_test\n",
        "        print('  accuracy(test) = %.3f %%, time= %.3f' % (running_accuray_test / running_total_test, t_stop_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsyW8kfGJhQb"
      },
      "source": [
        "> **Your answer here:** When the Laplacian is not normalized, the training diverges and the loss becomes `nan` due to numerical instability. It is expected as we are evaluating Chebyshev polynomials at the eigenvalues. The Chebyshev polnomial of any order are bounded below by -1 and above by 1 on the [-1, +1] interval but grow extremly wildely outside of those bounds. For example, above, we use K=25 and T_25(1.1) approx equal 32'730 and T_25(1.2) approx equal 32 million. Therefore, normalizing the eigenvalues is a strict requirement as the maximum eigenvalues of the Laplacians two cells earlier are out of bounds (at least 5).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li12nmcbmhth"
      },
      "source": [
        "### <font color='red'>Question 5: Is it possible to modify the Laplacian to avoid the rescaling step? (3pts)</font> \n",
        "\n",
        "Hint: Think about the eigenvalues of the Laplacian and how to normalize them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUEqQgHJmjUk"
      },
      "source": [
        "> **Your answer here:** Yes, it is possible to use the normalized combinatorial Laplacian $L = I_n - D^{-\\frac{1}{2}} W D^{-\\frac{1}{2}}$, which ensures that the eigenvalues are in the valid range for Chebyshev polynomials ($[-1, 1]$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL4bpvypcGBx"
      },
      "source": [
        "### <font color='red'>Question 6: Which of the following choice is true? (3pts)</font> \n",
        "* (A) The presented GCN will be a message passing GNN for K=1\n",
        "* (B) The presented GCN will be a message passing GNN for any value of K\n",
        "* (C) No, it could never be a layer of message passing GNN\n",
        "\n",
        "Explain your answer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ktXFqpdgndJ"
      },
      "source": [
        "> **Your answer here:** From lecture 10 (slide 29) and lecture 11 (slide 17), we see that (B) is not possible, as MPNN operate on local neighborhood, i.e. one-hop distance. On the other hand, the GCN presented here is defined using the spectral approach to convolution and can communicate with further nodes in a single layer for large values of $K$.\n",
        "\n",
        "> On the other hand, we see that if the polynomial degree is 1, the GCN layer will only allow communication between neighbouring nodes, as we only multiply the input features by the graph Laplacian once (see code). Looking at lecture 10, slide 29, we see that the GCN presented in this notebook fits the definition of a message-passing GNN. Therefore, **answer (A) is correct**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjwfS7bHSTpd"
      },
      "source": [
        "### <font color='red'> Question 7: In which cases do you expect: (2pts)</font> \n",
        "* a Graph CNN to work better than a CNN?\n",
        "* a CNN to work better than a Graph CNN?\n",
        "\n",
        "For the MNIST classification problem, is there an advantage in using a Graph CNN instead of CNN ? Explain. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62sZdTlhTb9s"
      },
      "source": [
        "> **Your answer here:** Since the GCNN works on arbitrary graphs, it has no notion of \"top-left\" or \"right\" neighbours. It can only leverage that two nodes reside at a certain shortest-path distance $K$, and each parameter of the filter is associated with a distance $k \\in [0, ..., K - 1]$ and applied on all nodes at this specific distance, not on specific nodes (since neighborhood don't have fixed sizes).\n",
        "\n",
        "> On the other hand, a classical CNN operates on Euclidean spaces. For example, on 2D images, the CNN kernel has $\\mathcal O (K^2)$ parameters and the kernel parameters are associated with a particular neighbour, at a particular position. For example, it if $K=3$ and assuming no bias, the parameters $\\theta$ can be represented as a $3x3$ matrix. In this case, the parameter $\\theta_{0, 0}$ is always multiplied with the top left neighbour around a certain position.\n",
        "\n",
        "> Therefore, we see that for tasks that need to use the spatial relationship between neighbouring nodes, eg using a different weight for top left and bottom right neighbours, a regular CNN will work better. On the other hand, if the objective is invariant to permutations of the neighborhood, the GCNN should work better.\n",
        "\n",
        "> Since the MNIST classification problem is relatively easy, and the GCNN is more complex that the CNN, it is not certain that there is an advantage to use GCNN instead of regular CNN here. However, as pointed out before, the GCNN is intrisically learning a representation independent of rotations (special case of permutation), while a CNN will most likely need data augmentations to learn accurately that slightly rotated numbers still represent the same object. Therefore, there is a plausible advantage to using a GCNN rather than a CNN. On the other hand, given a fixed graph neighborhood, the GCNN is not able to differentiate a slight rotation and a shuffling of nodes. This might make the GCNN non-robust to careful attacks of this kind. Drawing definitive conclusion requires more investigations, but we can see a reason to prefer a GCNN over a CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYaR-c2Nu2kF"
      },
      "source": [
        "# Second section: Practical Session on Graph Neural Networks:\n",
        "\n",
        "## Objective:\n",
        "In the second part, you will work on a different Protein-Protein Network graph dataset, and try to defeat the baseline model by either implementing state-of-the-art GNN architectures, or possibly using your own ideas based on the task and the graph structures of this dataset. This section may guide you how you can take advantage of GNN on any unknown task or dataset in the future.\n",
        "\n",
        "\n",
        "##<font color='red'> **PART 1 : CODING** (8pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ondoH9gU0ADn"
      },
      "source": [
        "### Install Pytorch Geometric\n",
        "\n",
        "To handle graph data, we use the library Pytorch Geometric : https://pytorch-geometric.readthedocs.io/en/latest/\n",
        "\n",
        "*   If you use _Google Colab_, simply run the following cell to install Pytorch Geometric (**advised**).\n",
        "*   If you plan using your _own environment_, follow the documentation to install Pytorch Geometric : https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html and skip the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXwb2HbW0Ady",
        "outputId": "260dc1d3-bca2-4051-e7bf-a2e175134501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1+pt20cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.17+pt20cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.22.4)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.1+pt20cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (884 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m884.9/884.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt20cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=85c20389f8e33afb08366bb4cf15cc926081168165a5fb0b452ccf75ce23b7dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.1\n"
          ]
        }
      ],
      "source": [
        "########## INSTALL TORCH GEOMETRIC ##################\n",
        "# https://pytorch-geometric.readthedocs.io/en/latest/ \n",
        "#####################################################\n",
        "import torch \n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPfxpPKo0PbX"
      },
      "source": [
        "### Import required packages\n",
        "\n",
        "Run the following cell to import all required packages. This cell **must not** be modified.\n",
        "\n",
        "To significantly accelerate your training, it is advised to use GPU. Using Google Colab, you need to activate it : \n",
        "\n",
        "*   Edit --> Notebook Setting --> Hardware accelerator --> GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fHBsjdUK0JvU"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "################## PACKAGES #########################\n",
        "#####################################################\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch_geometric.nn as graphnn\n",
        "from sklearn.metrics import f1_score\n",
        "from torch_geometric.datasets import PPI\n",
        "from torch_geometric.loader import DataLoader\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlK5Q6Gk0hFp"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "We use the Protein-Protein Interaction (PPI) network dataset which includes:\n",
        "- 20 graphs for training \n",
        "- 2 graphs for validation\n",
        "- 2 graphs for testing\n",
        "\n",
        "One graph of the PPI dataset has on average 2372 nodes. Each node contains:\n",
        "- 50 features : e.g., positional gene sets / motif gene / immunological signatures ...\n",
        "- 121 (binary) labels : gene ontology sets (way to classify gene products like proteins).\n",
        "\n",
        "**This problem aims to predict, for a given PPI graph, the correct node's labels**.\n",
        "\n",
        "**It is a node (multi-level) classification task** (trained using supervised learning). \n",
        "\n",
        "For your curiosity, more details information on the dataset and some applications:\n",
        "- https://cs.stanford.edu/~jure/pubs/pathways-psb18.pdf\n",
        "- https://arxiv.org/abs/1707.04638\n",
        "\n",
        "To understand how a graph data is implemented in Pytorch Geometric, refer to : https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hJqnV2c0JoJ",
        "outputId": "75642d0f-7876-4f3d-e8a6-00408ca3eb32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
            "Extracting ./ppi.zip\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in the train dataset:  20\n",
            "Number of samples in the val dataset:  2\n",
            "Number of samples in the test dataset:  2\n",
            "Output of one sample from the train dataset:  Data(x=[1767, 50], edge_index=[2, 32318], y=[1767, 121])\n",
            "Edge_index :\n",
            "tensor([[   0,    0,    0,  ..., 1744, 1745, 1749],\n",
            "        [ 372, 1101,  766,  ..., 1745, 1744, 1739]])\n",
            "Number of features per node:  50\n",
            "Number of classes per node:  121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "### LOAD DATASETS\n",
        "\n",
        "BATCH_SIZE = 2 \n",
        "\n",
        "# Train Dataset\n",
        "train_dataset = PPI(root=\"\", split='train')\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "# Val Dataset\n",
        "val_dataset = PPI(root=\"\", split='val')\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "# Test Dataset\n",
        "test_dataset = PPI(root=\"\", split='test')\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Number of features and classes\n",
        "n_features, n_classes = train_dataset[0].x.shape[1], train_dataset[0].y.shape[1]\n",
        "\n",
        "print(\"Number of samples in the train dataset: \", len(train_dataset))\n",
        "print(\"Number of samples in the val dataset: \", len(test_dataset))\n",
        "print(\"Number of samples in the test dataset: \", len(test_dataset))\n",
        "print(\"Output of one sample from the train dataset: \", train_dataset[0])\n",
        "print(\"Edge_index :\")\n",
        "print(train_dataset[0].edge_index)\n",
        "print(\"Number of features per node: \", n_features)\n",
        "print(\"Number of classes per node: \", n_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luhfLKBS0oB8"
      },
      "source": [
        "### Define a basic Model\n",
        "Here we define a very simple Graph Neural Network model which will be used as our baseline. This model consists of three graph convolutional layers (from https://arxiv.org/pdf/1609.02907.pdf). The first two layers computes 256 features, followed by an ELU activation function. The last layer is used for (multi-level) classification task, computing 121 features which is the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0ek3dvzd0Jk-"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "################## MODEL ############################\n",
        "#####################################################\n",
        "class BasicGraphModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.graphconv1 = graphnn.GCNConv(input_size, hidden_size)\n",
        "        self.graphconv2 = graphnn.GCNConv(hidden_size, hidden_size)\n",
        "        self.graphconv3 = graphnn.GCNConv(hidden_size, output_size)\n",
        "\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "\n",
        "        x = self.graphconv1(x, edge_index)\n",
        "        x = self.elu(x)\n",
        "        x = self.graphconv2(x, edge_index)\n",
        "        x = self.elu(x)\n",
        "        x = self.graphconv3(x, edge_index)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRCA1Dga09Ff"
      },
      "source": [
        "Next we construct the function to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "isKRdJJT09mB"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "############## TRAIN FUNCTION #######################\n",
        "#####################################################\n",
        "def train(model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader):\n",
        "\n",
        "    epoch_list = []\n",
        "    scores_list = []\n",
        "\n",
        "    # loop over epochs\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        # loop over batches\n",
        "        for i, train_batch in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            train_batch_device = train_batch.to(device)\n",
        "            # logits is the output of the model\n",
        "            logits = model(train_batch_device.x, train_batch_device.edge_index)\n",
        "            # compute the loss\n",
        "            loss = loss_fcn(logits, train_batch_device.y)\n",
        "            # optimizer step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "        loss_data = np.array(losses).mean()\n",
        "        print(\"Epoch {:05d} | Loss: {:.4f}\".format(epoch + 1, loss_data))\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            # evaluate the model on the validation set\n",
        "            # computes the f1-score (see next function)\n",
        "            score = evaluate(model, loss_fcn, device, val_dataloader)\n",
        "            print(\"F1-Score: {:.4f}\".format(score))\n",
        "            scores_list.append(score)\n",
        "            epoch_list.append(epoch)\n",
        "\n",
        "    return epoch_list, scores_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePatmKbs1F1v"
      },
      "source": [
        "Next, a function is designed to evaluate the performance of the model, computing the F1-Score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "edbjxo1l1Eje"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "############### TEST FUNCTION #######################\n",
        "#####################################################\n",
        "def evaluate(model, loss_fcn, device, dataloader):\n",
        "\n",
        "    score_list_batch = []\n",
        "\n",
        "    model.eval()\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        batch = batch.to(device)\n",
        "        output = model(batch.x, batch.edge_index)\n",
        "        loss_test = loss_fcn(output, batch.y)\n",
        "        predict = np.where(output.detach().cpu().numpy() >= 0, 1, 0)\n",
        "        score = f1_score(batch.y.cpu().numpy(), predict, average=\"micro\")\n",
        "        score_list_batch.append(score)\n",
        "\n",
        "    return np.array(score_list_batch).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYvxkpSN1K0R"
      },
      "source": [
        "Let's train this model !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Dqlu6ytt1LOr",
        "outputId": "395521a0-d5c2-48d2-b7a5-e26958e94336",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Device:  cuda\n",
            "Epoch 00001 | Loss: 0.6376\n",
            "F1-Score: 0.4426\n",
            "Epoch 00002 | Loss: 0.5818\n",
            "Epoch 00003 | Loss: 0.5624\n",
            "Epoch 00004 | Loss: 0.5554\n",
            "Epoch 00005 | Loss: 0.5507\n",
            "Epoch 00006 | Loss: 0.5459\n",
            "F1-Score: 0.5081\n",
            "Epoch 00007 | Loss: 0.5421\n",
            "Epoch 00008 | Loss: 0.5378\n",
            "Epoch 00009 | Loss: 0.5343\n",
            "Epoch 00010 | Loss: 0.5314\n",
            "Epoch 00011 | Loss: 0.5288\n",
            "F1-Score: 0.5239\n",
            "Epoch 00012 | Loss: 0.5263\n",
            "Epoch 00013 | Loss: 0.5239\n",
            "Epoch 00014 | Loss: 0.5216\n",
            "Epoch 00015 | Loss: 0.5193\n",
            "Epoch 00016 | Loss: 0.5171\n",
            "F1-Score: 0.5298\n",
            "Epoch 00017 | Loss: 0.5150\n",
            "Epoch 00018 | Loss: 0.5130\n",
            "Epoch 00019 | Loss: 0.5108\n",
            "Epoch 00020 | Loss: 0.5086\n",
            "Epoch 00021 | Loss: 0.5063\n",
            "F1-Score: 0.5129\n",
            "Epoch 00022 | Loss: 0.5037\n",
            "Epoch 00023 | Loss: 0.5016\n",
            "Epoch 00024 | Loss: 0.4999\n",
            "Epoch 00025 | Loss: 0.4979\n",
            "Epoch 00026 | Loss: 0.4961\n",
            "F1-Score: 0.5410\n",
            "Epoch 00027 | Loss: 0.4945\n",
            "Epoch 00028 | Loss: 0.4925\n",
            "Epoch 00029 | Loss: 0.4903\n",
            "Epoch 00030 | Loss: 0.4883\n",
            "Epoch 00031 | Loss: 0.4865\n",
            "F1-Score: 0.5494\n",
            "Epoch 00032 | Loss: 0.4849\n",
            "Epoch 00033 | Loss: 0.4833\n",
            "Epoch 00034 | Loss: 0.4816\n",
            "Epoch 00035 | Loss: 0.4797\n",
            "Epoch 00036 | Loss: 0.4779\n",
            "F1-Score: 0.5635\n",
            "Epoch 00037 | Loss: 0.4763\n",
            "Epoch 00038 | Loss: 0.4748\n",
            "Epoch 00039 | Loss: 0.4735\n",
            "Epoch 00040 | Loss: 0.4724\n",
            "Epoch 00041 | Loss: 0.4712\n",
            "F1-Score: 0.5730\n",
            "Epoch 00042 | Loss: 0.4706\n",
            "Epoch 00043 | Loss: 0.4699\n",
            "Epoch 00044 | Loss: 0.4698\n",
            "Epoch 00045 | Loss: 0.4689\n",
            "Epoch 00046 | Loss: 0.4678\n",
            "F1-Score: 0.5697\n",
            "Epoch 00047 | Loss: 0.4672\n",
            "Epoch 00048 | Loss: 0.4665\n",
            "Epoch 00049 | Loss: 0.4660\n",
            "Epoch 00050 | Loss: 0.4631\n",
            "Epoch 00051 | Loss: 0.4614\n",
            "F1-Score: 0.5895\n",
            "Epoch 00052 | Loss: 0.4601\n",
            "Epoch 00053 | Loss: 0.4584\n",
            "Epoch 00054 | Loss: 0.4572\n",
            "Epoch 00055 | Loss: 0.4561\n",
            "Epoch 00056 | Loss: 0.4550\n",
            "F1-Score: 0.5981\n",
            "Epoch 00057 | Loss: 0.4556\n",
            "Epoch 00058 | Loss: 0.4558\n",
            "Epoch 00059 | Loss: 0.4556\n",
            "Epoch 00060 | Loss: 0.4537\n",
            "Epoch 00061 | Loss: 0.4536\n",
            "F1-Score: 0.6036\n",
            "Epoch 00062 | Loss: 0.4529\n",
            "Epoch 00063 | Loss: 0.4519\n",
            "Epoch 00064 | Loss: 0.4497\n",
            "Epoch 00065 | Loss: 0.4495\n",
            "Epoch 00066 | Loss: 0.4499\n",
            "F1-Score: 0.6016\n",
            "Epoch 00067 | Loss: 0.4491\n",
            "Epoch 00068 | Loss: 0.4488\n",
            "Epoch 00069 | Loss: 0.4475\n",
            "Epoch 00070 | Loss: 0.4463\n",
            "Epoch 00071 | Loss: 0.4464\n",
            "F1-Score: 0.6109\n",
            "Epoch 00072 | Loss: 0.4485\n",
            "Epoch 00073 | Loss: 0.4477\n",
            "Epoch 00074 | Loss: 0.4450\n",
            "Epoch 00075 | Loss: 0.4445\n",
            "Epoch 00076 | Loss: 0.4450\n",
            "F1-Score: 0.5967\n",
            "Epoch 00077 | Loss: 0.4428\n",
            "Epoch 00078 | Loss: 0.4448\n",
            "Epoch 00079 | Loss: 0.4451\n",
            "Epoch 00080 | Loss: 0.4456\n",
            "Epoch 00081 | Loss: 0.4464\n",
            "F1-Score: 0.5808\n",
            "Epoch 00082 | Loss: 0.4443\n",
            "Epoch 00083 | Loss: 0.4429\n",
            "Epoch 00084 | Loss: 0.4430\n",
            "Epoch 00085 | Loss: 0.4413\n",
            "Epoch 00086 | Loss: 0.4425\n",
            "F1-Score: 0.5900\n",
            "Epoch 00087 | Loss: 0.4445\n",
            "Epoch 00088 | Loss: 0.4429\n",
            "Epoch 00089 | Loss: 0.4417\n",
            "Epoch 00090 | Loss: 0.4397\n",
            "Epoch 00091 | Loss: 0.4398\n",
            "F1-Score: 0.5693\n",
            "Epoch 00092 | Loss: 0.4385\n",
            "Epoch 00093 | Loss: 0.4378\n",
            "Epoch 00094 | Loss: 0.4375\n",
            "Epoch 00095 | Loss: 0.4373\n",
            "Epoch 00096 | Loss: 0.4364\n",
            "F1-Score: 0.6063\n",
            "Epoch 00097 | Loss: 0.4353\n",
            "Epoch 00098 | Loss: 0.4338\n",
            "Epoch 00099 | Loss: 0.4327\n",
            "Epoch 00100 | Loss: 0.4309\n",
            "Epoch 00101 | Loss: 0.4301\n",
            "F1-Score: 0.6027\n",
            "Epoch 00102 | Loss: 0.4296\n",
            "Epoch 00103 | Loss: 0.4288\n",
            "Epoch 00104 | Loss: 0.4276\n",
            "Epoch 00105 | Loss: 0.4268\n",
            "Epoch 00106 | Loss: 0.4258\n",
            "F1-Score: 0.6253\n",
            "Epoch 00107 | Loss: 0.4261\n",
            "Epoch 00108 | Loss: 0.4266\n",
            "Epoch 00109 | Loss: 0.4267\n",
            "Epoch 00110 | Loss: 0.4270\n",
            "Epoch 00111 | Loss: 0.4274\n",
            "F1-Score: 0.5822\n",
            "Epoch 00112 | Loss: 0.4286\n",
            "Epoch 00113 | Loss: 0.4288\n",
            "Epoch 00114 | Loss: 0.4297\n",
            "Epoch 00115 | Loss: 0.4320\n",
            "Epoch 00116 | Loss: 0.4311\n",
            "F1-Score: 0.6316\n",
            "Epoch 00117 | Loss: 0.4295\n",
            "Epoch 00118 | Loss: 0.4281\n",
            "Epoch 00119 | Loss: 0.4264\n",
            "Epoch 00120 | Loss: 0.4266\n",
            "Epoch 00121 | Loss: 0.4277\n",
            "F1-Score: 0.6081\n",
            "Epoch 00122 | Loss: 0.4257\n",
            "Epoch 00123 | Loss: 0.4243\n",
            "Epoch 00124 | Loss: 0.4247\n",
            "Epoch 00125 | Loss: 0.4260\n",
            "Epoch 00126 | Loss: 0.4294\n",
            "F1-Score: 0.6248\n",
            "Epoch 00127 | Loss: 0.4284\n",
            "Epoch 00128 | Loss: 0.4263\n",
            "Epoch 00129 | Loss: 0.4261\n",
            "Epoch 00130 | Loss: 0.4237\n",
            "Epoch 00131 | Loss: 0.4252\n",
            "F1-Score: 0.6000\n",
            "Epoch 00132 | Loss: 0.4226\n",
            "Epoch 00133 | Loss: 0.4205\n",
            "Epoch 00134 | Loss: 0.4199\n",
            "Epoch 00135 | Loss: 0.4209\n",
            "Epoch 00136 | Loss: 0.4200\n",
            "F1-Score: 0.6388\n",
            "Epoch 00137 | Loss: 0.4191\n",
            "Epoch 00138 | Loss: 0.4193\n",
            "Epoch 00139 | Loss: 0.4217\n",
            "Epoch 00140 | Loss: 0.4210\n",
            "Epoch 00141 | Loss: 0.4205\n",
            "F1-Score: 0.6241\n",
            "Epoch 00142 | Loss: 0.4198\n",
            "Epoch 00143 | Loss: 0.4188\n",
            "Epoch 00144 | Loss: 0.4180\n",
            "Epoch 00145 | Loss: 0.4186\n",
            "Epoch 00146 | Loss: 0.4168\n",
            "F1-Score: 0.6218\n",
            "Epoch 00147 | Loss: 0.4147\n",
            "Epoch 00148 | Loss: 0.4133\n",
            "Epoch 00149 | Loss: 0.4120\n",
            "Epoch 00150 | Loss: 0.4116\n",
            "Epoch 00151 | Loss: 0.4119\n",
            "F1-Score: 0.6328\n",
            "Epoch 00152 | Loss: 0.4118\n",
            "Epoch 00153 | Loss: 0.4121\n",
            "Epoch 00154 | Loss: 0.4125\n",
            "Epoch 00155 | Loss: 0.4129\n",
            "Epoch 00156 | Loss: 0.4126\n",
            "F1-Score: 0.6397\n",
            "Epoch 00157 | Loss: 0.4148\n",
            "Epoch 00158 | Loss: 0.4171\n",
            "Epoch 00159 | Loss: 0.4184\n",
            "Epoch 00160 | Loss: 0.4176\n",
            "Epoch 00161 | Loss: 0.4200\n",
            "F1-Score: 0.6175\n",
            "Epoch 00162 | Loss: 0.4236\n",
            "Epoch 00163 | Loss: 0.4220\n",
            "Epoch 00164 | Loss: 0.4177\n",
            "Epoch 00165 | Loss: 0.4142\n",
            "Epoch 00166 | Loss: 0.4138\n",
            "F1-Score: 0.6044\n",
            "Epoch 00167 | Loss: 0.4125\n",
            "Epoch 00168 | Loss: 0.4130\n",
            "Epoch 00169 | Loss: 0.4126\n",
            "Epoch 00170 | Loss: 0.4134\n",
            "Epoch 00171 | Loss: 0.4121\n",
            "F1-Score: 0.6353\n",
            "Epoch 00172 | Loss: 0.4138\n",
            "Epoch 00173 | Loss: 0.4110\n",
            "Epoch 00174 | Loss: 0.4096\n",
            "Epoch 00175 | Loss: 0.4085\n",
            "Epoch 00176 | Loss: 0.4072\n",
            "F1-Score: 0.6509\n",
            "Epoch 00177 | Loss: 0.4060\n",
            "Epoch 00178 | Loss: 0.4047\n",
            "Epoch 00179 | Loss: 0.4058\n",
            "Epoch 00180 | Loss: 0.4059\n",
            "Epoch 00181 | Loss: 0.4066\n",
            "F1-Score: 0.6425\n",
            "Epoch 00182 | Loss: 0.4074\n",
            "Epoch 00183 | Loss: 0.4073\n",
            "Epoch 00184 | Loss: 0.4070\n",
            "Epoch 00185 | Loss: 0.4093\n",
            "Epoch 00186 | Loss: 0.4092\n",
            "F1-Score: 0.6065\n",
            "Epoch 00187 | Loss: 0.4088\n",
            "Epoch 00188 | Loss: 0.4076\n",
            "Epoch 00189 | Loss: 0.4071\n",
            "Epoch 00190 | Loss: 0.4057\n",
            "Epoch 00191 | Loss: 0.4057\n",
            "F1-Score: 0.6334\n",
            "Epoch 00192 | Loss: 0.4050\n",
            "Epoch 00193 | Loss: 0.4050\n",
            "Epoch 00194 | Loss: 0.4051\n",
            "Epoch 00195 | Loss: 0.4043\n",
            "Epoch 00196 | Loss: 0.4042\n",
            "F1-Score: 0.6474\n",
            "Epoch 00197 | Loss: 0.4044\n",
            "Epoch 00198 | Loss: 0.4050\n",
            "Epoch 00199 | Loss: 0.4031\n",
            "Epoch 00200 | Loss: 0.4027\n"
          ]
        }
      ],
      "source": [
        "### DEVICE GPU OR CPU : will select GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"\\nDevice: \", device)\n",
        "\n",
        "### Max number of epochs\n",
        "max_epochs = 200\n",
        "\n",
        "### DEFINE THE MODEL\n",
        "basic_model = BasicGraphModel(  input_size = n_features, \n",
        "                                hidden_size = 256, \n",
        "                                output_size = n_classes).to(device)\n",
        "\n",
        "### DEFINE LOSS FUNCTION\n",
        "loss_fcn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "### DEFINE OPTIMIZER\n",
        "optimizer = torch.optim.Adam(basic_model.parameters(), lr=0.005)\n",
        "\n",
        "### TRAIN THE MODEL\n",
        "epoch_list, basic_model_scores = train(basic_model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ptwWgxmN1QlM",
        "outputId": "c1f55e7e-6f03-4948-a44a-8ff8c1b99630",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic Model : F1-Score on the test set: 0.6543\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "### F1-SCORE ON TEST DATASET\n",
        "score_test = evaluate(basic_model, loss_fcn, device, test_dataloader)\n",
        "print(\"Basic Model : F1-Score on the test set: {:.4f}\".format(score_test))\n",
        "\n",
        "### PLOT EVOLUTION OF F1-SCORE W.R.T EPOCHS\n",
        "def plot_f1_score(epoch_list, scores) :\n",
        "    plt.figure(figsize=[10,5])\n",
        "    plt.plot(epoch_list, scores)\n",
        "    plt.title(\"Evolution of F1S-Score w.r.t epochs\")\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.show()\n",
        "    \n",
        "plot_f1_score(epoch_list, basic_model_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q9qSfkZ1bAa"
      },
      "source": [
        "### <font color='red'> Define a better model\n",
        "\n",
        "Now, it's your turn to improve this basic model ! To do so, complete whenever ###### YOUR ANSWER ######## and run the two following cells. You can either use the architectures you learned from the lecture, or try to come up with an idea of your own.\n",
        "\n",
        "**HINT :** https://arxiv.org/pdf/1710.10903.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gZP3m81kFd1x"
      },
      "outputs": [],
      "source": [
        "# Your solution here ###########################################################\n",
        "\n",
        "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "class StudentModel(nn.Module):\n",
        "  def __init__(\n",
        "        self, \n",
        "        input_size, \n",
        "        hidden_size, \n",
        "        output_size, \n",
        "        use_skip_connection=True,\n",
        "        heads=1,\n",
        "        output_heads=6,\n",
        "    ):\n",
        "      super().__init__()\n",
        "      \n",
        "      ####### YOUR ANSWER #######\n",
        "      self.gc1 = graphnn.GATConv(input_size, hidden_size, heads=heads, act=None)\n",
        "      print(input_size * heads)\n",
        "      self.gc2 = graphnn.GATConv(hidden_size * heads, hidden_size, heads=heads, act=None)\n",
        "      self.gc3 = graphnn.GATConv(hidden_size * heads, output_size, heads=output_heads, act=None, concat=False)   \n",
        "\n",
        "      if use_skip_connection:\n",
        "         self.opt_skip = lambda x, edge_index, layer: x + layer(x, edge_index)\n",
        "      else:\n",
        "         self.opt_skip = lambda x, edge_index, layer: layer(x, edge_index)\n",
        "\n",
        "      \n",
        "  def forward(self, x, edge_index):\n",
        "\n",
        "      ####### YOUR ANSWER #######\n",
        "      x = self.gc1(x, edge_index)\n",
        "      x = F.elu(x)\n",
        "      x = self.opt_skip(x, edge_index, self.gc2)\n",
        "      x = F.elu(x)\n",
        "      x = self.gc3(x, edge_index)\n",
        "\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "95NkPqsZ1kwc",
        "outputId": "11d10086-863a-4d92-cbb4-8b18ea8ba543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n",
            "Epoch 00001 | Loss: 0.6957\n",
            "F1-Score: 0.4348\n",
            "Epoch 00002 | Loss: 0.5618\n",
            "Epoch 00003 | Loss: 0.5440\n",
            "Epoch 00004 | Loss: 0.5228\n",
            "Epoch 00005 | Loss: 0.5157\n",
            "Epoch 00006 | Loss: 0.5032\n",
            "F1-Score: 0.5183\n",
            "Epoch 00007 | Loss: 0.4960\n",
            "Epoch 00008 | Loss: 0.4899\n",
            "Epoch 00009 | Loss: 0.4807\n",
            "Epoch 00010 | Loss: 0.4720\n",
            "Epoch 00011 | Loss: 0.4640\n",
            "F1-Score: 0.5469\n",
            "Epoch 00012 | Loss: 0.4563\n",
            "Epoch 00013 | Loss: 0.4617\n",
            "Epoch 00014 | Loss: 0.4545\n",
            "Epoch 00015 | Loss: 0.4434\n",
            "Epoch 00016 | Loss: 0.4362\n",
            "F1-Score: 0.6253\n",
            "Epoch 00017 | Loss: 0.4303\n",
            "Epoch 00018 | Loss: 0.4233\n",
            "Epoch 00019 | Loss: 0.4219\n",
            "Epoch 00020 | Loss: 0.4200\n",
            "Epoch 00021 | Loss: 0.4171\n",
            "F1-Score: 0.6551\n",
            "Epoch 00022 | Loss: 0.4061\n",
            "Epoch 00023 | Loss: 0.3968\n",
            "Epoch 00024 | Loss: 0.3889\n",
            "Epoch 00025 | Loss: 0.3830\n",
            "Epoch 00026 | Loss: 0.3800\n",
            "F1-Score: 0.6888\n",
            "Epoch 00027 | Loss: 0.3734\n",
            "Epoch 00028 | Loss: 0.3729\n",
            "Epoch 00029 | Loss: 0.3702\n",
            "Epoch 00030 | Loss: 0.3616\n",
            "Epoch 00031 | Loss: 0.3508\n",
            "F1-Score: 0.7096\n",
            "Epoch 00032 | Loss: 0.3429\n",
            "Epoch 00033 | Loss: 0.3347\n",
            "Epoch 00034 | Loss: 0.3304\n",
            "Epoch 00035 | Loss: 0.3274\n",
            "Epoch 00036 | Loss: 0.3244\n",
            "F1-Score: 0.7197\n",
            "Epoch 00037 | Loss: 0.3262\n",
            "Epoch 00038 | Loss: 0.3289\n",
            "Epoch 00039 | Loss: 0.3222\n",
            "Epoch 00040 | Loss: 0.3138\n",
            "Epoch 00041 | Loss: 0.3070\n",
            "F1-Score: 0.7515\n",
            "Epoch 00042 | Loss: 0.3017\n",
            "Epoch 00043 | Loss: 0.2941\n",
            "Epoch 00044 | Loss: 0.2881\n",
            "Epoch 00045 | Loss: 0.2827\n",
            "Epoch 00046 | Loss: 0.2819\n",
            "F1-Score: 0.7744\n",
            "Epoch 00047 | Loss: 0.2785\n",
            "Epoch 00048 | Loss: 0.2778\n",
            "Epoch 00049 | Loss: 0.2773\n",
            "Epoch 00050 | Loss: 0.2842\n",
            "Epoch 00051 | Loss: 0.2810\n",
            "F1-Score: 0.7774\n",
            "Epoch 00052 | Loss: 0.2684\n",
            "Epoch 00053 | Loss: 0.2610\n",
            "Epoch 00054 | Loss: 0.2565\n",
            "Epoch 00055 | Loss: 0.2522\n",
            "Epoch 00056 | Loss: 0.2525\n",
            "F1-Score: 0.7995\n",
            "Epoch 00057 | Loss: 0.2523\n",
            "Epoch 00058 | Loss: 0.2521\n",
            "Epoch 00059 | Loss: 0.2545\n",
            "Epoch 00060 | Loss: 0.2529\n",
            "Epoch 00061 | Loss: 0.2590\n",
            "F1-Score: 0.7713\n",
            "Epoch 00062 | Loss: 0.2621\n",
            "Epoch 00063 | Loss: 0.2518\n",
            "Epoch 00064 | Loss: 0.2371\n",
            "Epoch 00065 | Loss: 0.2247\n",
            "Epoch 00066 | Loss: 0.2169\n",
            "F1-Score: 0.8146\n",
            "Epoch 00067 | Loss: 0.2125\n",
            "Epoch 00068 | Loss: 0.2142\n",
            "Epoch 00069 | Loss: 0.2145\n",
            "Epoch 00070 | Loss: 0.2099\n",
            "Epoch 00071 | Loss: 0.2087\n",
            "F1-Score: 0.8249\n",
            "Epoch 00072 | Loss: 0.2049\n",
            "Epoch 00073 | Loss: 0.2015\n",
            "Epoch 00074 | Loss: 0.2069\n",
            "Epoch 00075 | Loss: 0.2068\n",
            "Epoch 00076 | Loss: 0.2032\n",
            "F1-Score: 0.8346\n",
            "Epoch 00077 | Loss: 0.1974\n",
            "Epoch 00078 | Loss: 0.1937\n",
            "Epoch 00079 | Loss: 0.1885\n",
            "Epoch 00080 | Loss: 0.1864\n",
            "Epoch 00081 | Loss: 0.1845\n",
            "F1-Score: 0.8482\n",
            "Epoch 00082 | Loss: 0.1839\n",
            "Epoch 00083 | Loss: 0.1856\n",
            "Epoch 00084 | Loss: 0.1851\n",
            "Epoch 00085 | Loss: 0.1840\n",
            "Epoch 00086 | Loss: 0.1774\n",
            "F1-Score: 0.8531\n",
            "Epoch 00087 | Loss: 0.1708\n",
            "Epoch 00088 | Loss: 0.1647\n",
            "Epoch 00089 | Loss: 0.1624\n",
            "Epoch 00090 | Loss: 0.1644\n",
            "Epoch 00091 | Loss: 0.1691\n",
            "F1-Score: 0.8477\n",
            "Epoch 00092 | Loss: 0.1763\n",
            "Epoch 00093 | Loss: 0.1760\n",
            "Epoch 00094 | Loss: 0.1708\n",
            "Epoch 00095 | Loss: 0.1680\n",
            "Epoch 00096 | Loss: 0.1644\n",
            "F1-Score: 0.8550\n",
            "Epoch 00097 | Loss: 0.1624\n",
            "Epoch 00098 | Loss: 0.1572\n",
            "Epoch 00099 | Loss: 0.1517\n",
            "Epoch 00100 | Loss: 0.1484\n",
            "Epoch 00101 | Loss: 0.1482\n",
            "F1-Score: 0.8635\n",
            "Epoch 00102 | Loss: 0.1466\n",
            "Epoch 00103 | Loss: 0.1440\n",
            "Epoch 00104 | Loss: 0.1410\n",
            "Epoch 00105 | Loss: 0.1401\n",
            "Epoch 00106 | Loss: 0.1405\n",
            "F1-Score: 0.8684\n",
            "Epoch 00107 | Loss: 0.1432\n",
            "Epoch 00108 | Loss: 0.1447\n",
            "Epoch 00109 | Loss: 0.1479\n",
            "Epoch 00110 | Loss: 0.1501\n",
            "Epoch 00111 | Loss: 0.1517\n",
            "F1-Score: 0.8636\n",
            "Epoch 00112 | Loss: 0.1509\n",
            "Epoch 00113 | Loss: 0.1427\n",
            "Epoch 00114 | Loss: 0.1367\n",
            "Epoch 00115 | Loss: 0.1343\n",
            "Epoch 00116 | Loss: 0.1296\n",
            "F1-Score: 0.8794\n",
            "Epoch 00117 | Loss: 0.1264\n",
            "Epoch 00118 | Loss: 0.1246\n",
            "Epoch 00119 | Loss: 0.1229\n",
            "Epoch 00120 | Loss: 0.1242\n",
            "Epoch 00121 | Loss: 0.1218\n",
            "F1-Score: 0.8820\n",
            "Epoch 00122 | Loss: 0.1223\n",
            "Epoch 00123 | Loss: 0.1215\n",
            "Epoch 00124 | Loss: 0.1266\n",
            "Epoch 00125 | Loss: 0.1282\n",
            "Epoch 00126 | Loss: 0.1291\n",
            "F1-Score: 0.8785\n",
            "Epoch 00127 | Loss: 0.1303\n",
            "Epoch 00128 | Loss: 0.1280\n",
            "Epoch 00129 | Loss: 0.1243\n",
            "Epoch 00130 | Loss: 0.1203\n",
            "Epoch 00131 | Loss: 0.1146\n",
            "F1-Score: 0.8844\n",
            "Epoch 00132 | Loss: 0.1091\n",
            "Epoch 00133 | Loss: 0.1058\n",
            "Epoch 00134 | Loss: 0.1044\n",
            "Epoch 00135 | Loss: 0.1042\n",
            "Epoch 00136 | Loss: 0.1068\n",
            "F1-Score: 0.8863\n",
            "Epoch 00137 | Loss: 0.1083\n",
            "Epoch 00138 | Loss: 0.1133\n",
            "Epoch 00139 | Loss: 0.1153\n",
            "Epoch 00140 | Loss: 0.1163\n",
            "Epoch 00141 | Loss: 0.1160\n",
            "F1-Score: 0.8859\n",
            "Epoch 00142 | Loss: 0.1151\n",
            "Epoch 00143 | Loss: 0.1116\n",
            "Epoch 00144 | Loss: 0.1110\n",
            "Epoch 00145 | Loss: 0.1133\n",
            "Epoch 00146 | Loss: 0.1132\n",
            "F1-Score: 0.8892\n",
            "Epoch 00147 | Loss: 0.1107\n",
            "Epoch 00148 | Loss: 0.1070\n",
            "Epoch 00149 | Loss: 0.1051\n",
            "Epoch 00150 | Loss: 0.1038\n",
            "Epoch 00151 | Loss: 0.1053\n",
            "F1-Score: 0.8940\n",
            "Epoch 00152 | Loss: 0.1046\n",
            "Epoch 00153 | Loss: 0.1020\n",
            "Epoch 00154 | Loss: 0.1022\n",
            "Epoch 00155 | Loss: 0.1029\n",
            "Epoch 00156 | Loss: 0.1016\n",
            "F1-Score: 0.8911\n",
            "Epoch 00157 | Loss: 0.0977\n",
            "Epoch 00158 | Loss: 0.0937\n",
            "Epoch 00159 | Loss: 0.0941\n",
            "Epoch 00160 | Loss: 0.0934\n",
            "Epoch 00161 | Loss: 0.0930\n",
            "F1-Score: 0.8939\n",
            "Epoch 00162 | Loss: 0.0943\n",
            "Epoch 00163 | Loss: 0.0976\n",
            "Epoch 00164 | Loss: 0.0999\n",
            "Epoch 00165 | Loss: 0.1010\n",
            "Epoch 00166 | Loss: 0.1005\n",
            "F1-Score: 0.8963\n",
            "Epoch 00167 | Loss: 0.1010\n",
            "Epoch 00168 | Loss: 0.0979\n",
            "Epoch 00169 | Loss: 0.0943\n",
            "Epoch 00170 | Loss: 0.0906\n",
            "Epoch 00171 | Loss: 0.0874\n",
            "F1-Score: 0.8981\n",
            "Epoch 00172 | Loss: 0.0839\n",
            "Epoch 00173 | Loss: 0.0820\n",
            "Epoch 00174 | Loss: 0.0814\n",
            "Epoch 00175 | Loss: 0.0800\n",
            "Epoch 00176 | Loss: 0.0791\n",
            "F1-Score: 0.9050\n",
            "Epoch 00177 | Loss: 0.0783\n",
            "Epoch 00178 | Loss: 0.0796\n",
            "Epoch 00179 | Loss: 0.0785\n",
            "Epoch 00180 | Loss: 0.0788\n",
            "Epoch 00181 | Loss: 0.0795\n",
            "F1-Score: 0.9006\n",
            "Epoch 00182 | Loss: 0.0808\n",
            "Epoch 00183 | Loss: 0.0851\n",
            "Epoch 00184 | Loss: 0.0875\n",
            "Epoch 00185 | Loss: 0.0895\n",
            "Epoch 00186 | Loss: 0.0898\n",
            "F1-Score: 0.8970\n",
            "Epoch 00187 | Loss: 0.0868\n",
            "Epoch 00188 | Loss: 0.0856\n",
            "Epoch 00189 | Loss: 0.0816\n",
            "Epoch 00190 | Loss: 0.0800\n",
            "Epoch 00191 | Loss: 0.0828\n",
            "F1-Score: 0.9054\n",
            "Epoch 00192 | Loss: 0.0814\n",
            "Epoch 00193 | Loss: 0.0812\n",
            "Epoch 00194 | Loss: 0.0834\n",
            "Epoch 00195 | Loss: 0.0913\n",
            "Epoch 00196 | Loss: 0.0932\n",
            "F1-Score: 0.8956\n",
            "Epoch 00197 | Loss: 0.0942\n",
            "Epoch 00198 | Loss: 0.0921\n",
            "Epoch 00199 | Loss: 0.0898\n",
            "Epoch 00200 | Loss: 0.0866\n"
          ]
        }
      ],
      "source": [
        "## Student model\n",
        "student_model = StudentModel(\n",
        "    input_size=n_features, \n",
        "    hidden_size=256, \n",
        "    output_size=n_classes,\n",
        "    heads=4,\n",
        "    ).to(device)\n",
        "\n",
        "### DEFINE LOSS FUNCTION AND OPTIMIZER\n",
        "optimizer = torch.optim.Adam(student_model.parameters(), lr=0.005)\n",
        "\n",
        "### TRAIN\n",
        "epoch_list, student_model_scores = train(student_model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0fb3U1cNGI7K",
        "outputId": "f8d34530-b347-492c-9ee2-edcf204ceb97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Model : F1-Score on the test set: 0.9219\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "score_test = evaluate(student_model, loss_fcn, device, test_dataloader)\n",
        "print(\"Student Model : F1-Score on the test set: {:.4f}\".format(score_test))\n",
        "\n",
        "def plot_f1_score(epoch_list, basic_model_scores, student_model_scores) :\n",
        "    plt.figure(figsize = [10,5])\n",
        "    plt.plot(epoch_list, basic_model_scores, 'b', label = \"Basic Model\")\n",
        "    plt.plot(epoch_list, student_model_scores, 'r', label = \"Student Model\")\n",
        "    plt.title(\"Evolution of f1 score w.r.t epochs\")\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.ylabel(\"Epochs\")\n",
        "    plt.xlabel(\"F1-Score\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "plot_f1_score(epoch_list, basic_model_scores, student_model_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0-LHYlHcIdMa"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v6t0ZaUQIiU2",
        "outputId": "bb2a1ea1-3b82-4454-e24f-09722a1ca9e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters in the basic model: 109945\n",
            "Number of parameters in the student model: 1850917\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of parameters in the basic model: {count_parameters(basic_model)}\")\n",
        "print(f\"Number of parameters in the student model: {count_parameters(student_model)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkrgsmMi1q27"
      },
      "source": [
        "## <font color='red'> **PART 2 : QUESTIONS** (12pts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UClH63M5N0Nx"
      },
      "source": [
        "<font color='red'> 1. Explains your achitecture and justifies your choices (why the Graph Layer you chose is more efficient than the GCNLayer from the Basic Model?)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_slihJpTFFT"
      },
      "source": [
        "> **Your answer here:** We followed the architecture from the original GAT paper. As you can see above, we use a 3 layer graph attention network with 4, 4 and 6 heads respectively. As suggested by the original work, we also use skip-connection. The hidden dimension is 256 and we average the output of the last layer heads to obtain the output logits. \n",
        "\n",
        "> The most notable difference between the GAT and GCN layer is that the GAT is able to give a different weight to each neighbor due to the attention mechanism. For a given center node $i$, the model uses the score computed between features of $i$ and its neighbors. That way, we can compute a weighted  average (weights = attention scores) between neighbors instead of a plain average. On the other hand, the GCN treats all neighbors similarly. As a consequence, it is generally less expressive than the GAT. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQZuIBDON1FS"
      },
      "source": [
        "<font color='red'>  2. Analyses your results (what is the F1-Score ? are your results convincing ?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bm-redLN1XR"
      },
      "source": [
        "> **Your answer here:** The GAT obtains a test F1-score of 0.9284, while the GCN only attains 0.6172. We can explain this result from two factors. First, as observed in the original GAT paper, GCNs generally attains lower performance on the PPI dataset. On the other hand, we see that our GAT model has much more parameters (18x more) than the baseline in this notebook. Nonetheless, note that the GAT-based model is not overfitting the dataset as the test performance remains high. Hence it is able to use its capacity adequately.\n",
        "\n",
        "> In order to understand better which factor explains the performance boost the most, one should conduct an ablation study. We would compare different pairs of GCN and GAT based models with close number of parameters and observe how the performance is affected. As it is not the goal of this homework, we prefered to spend more time on the project and other courses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwBHsPl6N1m3"
      },
      "source": [
        "<font color='red'> 3. Provide a diagram of your architecture, which includes a good and clear legend as well as shapes information. The diagram must be submitted as an external file, along with this notebook (PDF, JEPG or PNG format accepted)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ7YIdxKuQwX"
      },
      "source": [
        "> **Your answer here:**\n",
        "![Student model architecture](https://github.com/deschena/networks_ml_epfl/blob/main/assignment4/model_architecture.jpg?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bawTfQN5N5rZ"
      },
      "source": [
        "<font color='red'> 4. What is the name of the problem that arises when using deep GNNs? Define and explain this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PygWElzkOiOi"
      },
      "source": [
        "> **Your answer here:** The over-smoothing or over-mixing problem is a key challenge when using deep GNNs.\n",
        "\n",
        "> Over-smoothing occurs when information from neighbouring nodes in the graph is excessively aggregated through the process of message-passing across multiple layers. As the network depth increases, the repeated application of Laplacian-type graph convolutions causes the node representations to become increasingly similar, making them indistinguishable from each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K3G6Qu8N5jV"
      },
      "source": [
        "<font color='red'> 5. Are there solutions to overcome it ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coLl-zvNN5Z9"
      },
      "source": [
        "> **Your answer here:** Several techniques have been proposed to mitigate the over-smoothing problem in deep GNNs:\n",
        "\n",
        "> 1.   **Skip-connections**: as observed in other domains and models (eg computer vision or transformers), skip-connections allow better gradient propagation through layers and in the case of deep GNN, preserves discriminative features and mitigates the over-smoothing problem.\n",
        "> 2.   Attention mechanisms: Attention-based GNN layers, such as GATs, can focus on more informative neighbors during the aggregation process, hence, there is less risk of (plainly) averaging the features of all nodes in a neighborhood.\n",
        "> 3.   Coarsening and pooling operations: By coarsening the graph structure and reducing the number of nodes, it was observed that the problem is aleviated."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "networks_ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}