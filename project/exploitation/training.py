import numpy as np
from matplotlib import pyplot as plt
from sklearn.metrics import f1_score
import torch
from tqdm import tqdm
from einops import rearrange, reduce
from torch_geometric.loader import DataLoader
from torch import nn
from sklearn.cluster import SpectralClustering, AffinityPropagation, KMeans
from grakel import Graph, GraphKernel
import networkx as nx
import xgboost as xgb


# ================================================
# ============ Pytorch geometric part ============
# ================================================


def eval_regression_model(
        model, device, valid_loader, feats_mu, feats_std, targets_mu, targets_std
):
    """Evaluate regression model.

    Args:
        model (torch.nn): Pytorch geometric regression model.
        device (str): Device to use.
        valid_loader (DataLoader): Validation Dataloader.
        feats_mu (torch.array): Array of features mean.
        feats_std (torch.array): Array of features std.
        targets_mu (torch.array): Array of targets mean.
        targets_std (torch.array): Array of targets std.

    Returns:
        dict: Dictionary of metrics computed on the validation set.
    """
    # TODO: add other metrics here
    mse = 0
    mae = 0
    n = 0
    model = model.eval()
    with torch.no_grad():
        for batch in valid_loader:
            x = batch.x.to(device)
            y = batch.y.to(device)

            x = (x - feats_mu) / feats_std
            y = (y - targets_mu) / targets_std
            edge_index = batch.edge_index.to(device)

            preds = model(x, edge_index).squeeze()
            mse += torch.mean((y - preds) ** 2)
            mae += torch.mean(torch.abs(y - preds))
            n += 1

    return {
        "mse": mse / n,
        "mae": mae / n,
    }


def _accuracy(preds, targets, thr=0.5):
    rounded_preds = (preds >= thr).to(int)
    correct = (rounded_preds == targets).sum().item()
    total = targets.size(0)
    accuracy = correct / total

    return accuracy


def _f1_score(preds, targets, thr=0.5):
    rounded_preds = (preds >= thr).to(int)
    np_preds = rounded_preds.detach().cpu().numpy()
    np_targets = targets.detach().cpu().numpy()
    f1 = f1_score(np_targets, np_preds)

    return f1


def eval_classification_model(
        model, device, valid_loader, feats_mu, feats_std
):
    """Evaluate PyG classification model

    Args:
        model (torch.nn): PyG classification model
        device (str): Device to use
        valid_loader (DataLoader): PyG validation loader
        feats_mu (torch.tensor): Per feature average feature
        feats_std (torch.tensor): Per feature standard deviation

    Returns:
        dict: Dictionary with accuracy and F1-score on validation set
    """
    acc = 0
    f1 = 0
    n = 0
    model.eval()
    with torch.no_grad():
        for batch in valid_loader:
            x = batch.x.to(device)
            y = batch.y.to(device)

            x = (x - feats_mu) / feats_std
            edge_index = batch.edge_index.to(device)

            preds = model(x, edge_index).squeeze()

            acc += _accuracy(preds, y)
            f1 += _f1_score(preds, y)
            n += 1

    return {
        "accuracy": acc / n,
        "f1-score": f1 / n,
    }


def _pyg_inp_feats_stats(list_):
    """Compute mean and std of a list of features.

    Args:
        list_ (list): List of features in torch tensor.

    Returns:
        (torch.tensor, torch.tensor): Mean and std over features.
    """
    concat = torch.concat(list_, dim=0)
    m = torch.mean(concat, axis=0, keepdim=True)
    std = torch.std(concat, axis=0, keepdim=True)
    return m, std


def _pyg_postprocess_eval_metrics(metrics_list, log_every, num_steps):
    """Postprocess the evaluation metrics for easier plotting

    Args:
        metrics_list (list): List of per eval step metrics as dict. Turn into a dict of numpy arrays
        log_every (int): How often do we log the metrics
        num_steps (int): Total number of steps

    Returns:
        dict: Dictionary of metrics and logging steps (as `x`)
    """
    if len(metrics_list) == 0:
        return metrics_list

    values = {k: [] for k in metrics_list[0].keys()}

    for m in metrics_list:
        for metric in values.keys():
            values[metric].append(m[metric])

    xs = np.arange(log_every, num_steps + log_every, log_every)
    # values = {k: np.array(v) for k, v in values.items()}
    values = {k: np.array([item.item() if torch.is_tensor(item) else item for item in v]) for k, v in values.items()}
    values["x"] = xs

    return values


def pyg_train(
        model,
        device,
        train_split,
        valid_split,
        optimizer,
        criterion,
        batch_size=2,
        num_steps=100,
        eval_every=100,
        normalize_features=True,
        normalize_targets=True,
        regression=True,  # Determines what validation loop to use
        verbose=True,
):
    """Pytorch geometric main training loop.

    Args:
        model (torch.nn): Torch geometric model.
        device (str): Device to use.
        train_split (list): List of training cities.
        valid_split (list): List of validation cities.
        optimizer (torch.nn.optim): Optimizer to update model's weights.
        criterion (Callable): Differentiable criterion to compute gradient wrt parameter's weights.
        batch_size (int, optional): Train/Valid loader's batch size. Defaults to 2.
        num_steps (int, optional): Number of parameter's update. Defaults to 100.
        eval_every (int, optional): Number of training steps before logging. Defaults to 100.
        normalize_features (bool, optional): Whether to normalize input features. Defaults to True.
        normalize_targets (bool, optional): Whether to normalize targets. Good for regression. Defaults to True.
        regression (bool, optional): Determines which validation loop to use between regression and classification. Default to True
        verbose (bool, optional): Display progress with a tqdm loading bar. Defaults to True.

    Returns:
        dict: Training results
    """
    if normalize_features:
        feats_m, feats_std = _pyg_inp_feats_stats([d.x for d in train_split])
        feats_m = feats_m.to(device)
        feats_std = feats_std.to(device)
    else:
        feats_m = 0
        feats_std = 1

    if not regression:  # normalise target not necessary for classification
        normalize_targets = False

    if normalize_targets:
        targets_m, targets_std = _pyg_inp_feats_stats([d.y for d in train_split])
        targets_m = targets_m.to(device)
        targets_std = targets_std.to(device)
    else:
        targets_m = 0
        targets_std = 1

    train_loader = DataLoader(train_split, batch_size=batch_size)
    valid_loader = DataLoader(valid_split, batch_size=batch_size)

    model = model.to(device)

    step_idx = 0
    pbar = tqdm(
        total=num_steps, desc=f"{'Regression' if regression else 'Classification'} train...", disable=not verbose,
        leave=True
    )

    loss_array = []
    eval_metrics = []

    while True:
        for batch in train_loader:
            model.train()

            x = batch.x.to(device)
            y = batch.y.to(device)

            x = (x - feats_m) / feats_std
            y = (y - targets_m) / targets_std
            edge_index = batch.edge_index.to(device)

            preds = model(x, edge_index)
            preds = preds.squeeze()
            optimizer.zero_grad()
            loss = criterion(preds, y)
            loss.backward()
            optimizer.step()
            loss_array.append(loss.detach().cpu().item())

            step_idx += 1
            pbar.update(1)

            if step_idx % eval_every == 0:
                if regression:
                    metrics = eval_regression_model(
                        model,
                        device,
                        valid_loader,
                        feats_m,
                        feats_std,
                        targets_m,
                        targets_std,
                    )
                else:
                    metrics = eval_classification_model(
                        model,
                        device,
                        valid_loader,
                        feats_m,
                        feats_std,
                    )

                eval_metrics.append(metrics)

            if step_idx == num_steps:
                pbar.close()
                return {
                    "loss": loss_array,
                    "eval_metrics": _pyg_postprocess_eval_metrics(eval_metrics, eval_every, num_steps),
                    "feats_m": feats_m,
                    "feats_std": feats_std,
                    "targets_m": targets_m,
                    "targets_std": targets_std,
                }


# ================================================
# ============== Classical ML part ===============
# ================================================


def sklearn_eval(model, features, targets, metric):
    """Compute predetermined metric.

    Args:
        model (): Model with sklearn-like API.
        features (np.ndarray): Features array.
        targets (np.ndarray): Targets array.
        metric (str): Metric to compute.

    Returns:
        np.ndarray: Singleton array containing the metric of interest.
    """
    preds = model.predict(features)

    if metric == "mse":
        return np.mean((preds - targets) ** 2)

    elif metric == "mae":
        return np.mean(np.abs(preds - targets))

    elif metric == "accuracy":
        return np.isclose(targets, preds).mean()

    elif metric == "f1":
        return f1_score(targets, preds)

    else:
        raise ValueError(f"Unknown metric {metric}")


def _sklearn_data_preprocess_one(data, edge_aggr):
    """Preprocess PyG's Data object into edge features.

    Args:
        data (Data): PyG's Data object.
        edge_aggr (str): Edge aggregation method (features live on nodes originally).

    Returns:
        (np.ndarray, np.ndarray): (Edge features, edge targets).
    """
    x_edges = data.x[data.edge_index, :]

    if edge_aggr == "concat":
        x_edges = rearrange(x_edges, "two n_sample n_feats -> n_sample (two n_feats)")
    else:  # sum, mean, max, min
        x_edges = reduce(x_edges, "two n_sample n_feats -> n_sample n_feats", edge_aggr)

    return x_edges, data.y


def sklearn_data_preprocess(data_split, edge_aggr):
    """Preprocess a list of features/targets into a numpy array.

    Args:
        data_split (list): List of PyG's Data object to concat into a numpy array.
        edge_aggr (str): Edge aggregation method.

    Returns:
        (np.ndarray, np.ndarray): Concatenated pair of edge features and target features.
    """
    processed_split = [
        _sklearn_data_preprocess_one(data, edge_aggr) for data in data_split
    ]
    feats, targets = zip(*processed_split)
    feats = np.concatenate(feats, axis=0)
    targets = np.concatenate(targets, axis=0)
    return feats, targets


def sklearn_train(
        model,
        train_split,
        valid_split,
        edge_aggr: str = "concat",
        normalize_inputs=True,
        normalize_targets=True,
        train_criterions=["mse", "mae"],
        valid_criterions=["mse", "mae"],
        verbose=True,
):
    """Train model with sklearn API.

    Args:
        model (): Model with sklearn-like API (model.fit, model.predict, ...).
        train_split (list): List of PyG's Data to use for training.
        valid_split (list): List of PyG's Data to use for validation.
        edge_aggr (str, optional): Edge aggregation method. Valid values are "concat", "sum", "mean", "max", "min". Defaults to "concat".
        normalize_inputs (bool, optional): Whether to normalize inputs. Defaults to True.
        normalize_targets (bool, optional): Whether to normalize targets. Defaults to True.
        train_criterions (list, optional): List of criterions to measure on train data. Defaults to ["mse"].
        valid_criterions (list, optional): List of criterions to measure on validation data. Defaults to ["mse"].
        verbose (bool, optional): Whether to print about the training process. Defaults to True.

    Returns:
        dict: Dictionary of results.
    """

    assert edge_aggr in ("concat", "sum", "mean", "max", "min")
    # Prepare data
    if verbose:
        print("Preparing data...")
    train_feats, train_targets = sklearn_data_preprocess(train_split, edge_aggr)
    valid_feats, valid_targets = sklearn_data_preprocess(valid_split, edge_aggr)

    if normalize_inputs:
        feats_m, feats_std = np.mean(train_feats, axis=0, keepdims=True), np.std(
            train_feats, axis=0, keepdims=True
        )
    else:
        feats_m, feats_std = 0, 1

    if normalize_targets:
        targets_m, targets_std = np.mean(train_targets, axis=0, keepdims=True), np.std(
            train_targets, axis=0, keepdims=True
        )
    else:
        targets_m, targets_std = 0, 1

    train_feats = (train_feats - feats_m) / feats_std
    valid_feats = (valid_feats - feats_m) / feats_std

    train_targets = (train_targets - targets_m) / targets_std
    valid_targets = (valid_targets - targets_m) / targets_std

    if verbose:
        print("Data prepared")
        print("Training model...")

    model.fit(train_feats, train_targets)
    if verbose:
        print("Model trained.")
        print("Evaluation model...")

    train_metrics = {
        m: sklearn_eval(model, train_feats, train_targets, m) for m in train_criterions
    }
    valid_metrics = {
        m: sklearn_eval(model, valid_feats, valid_targets, m) for m in valid_criterions
    }
    if verbose:
        print("Model evaluated")

    return {
        "train_metrics": train_metrics,
        "valid_metrics": valid_metrics,
        "feats_m": feats_m,
        "feats_std": feats_std,
        "targets_m": targets_m,
        "targets_std": targets_std,
    }


# ================================================
# ============== Graph Kernel part ===============
# ================================================


def compute_graph_kernels(graph_list, n_samples=10):
    """Compute graph kernels for a list of graphs.

    Args:
        graph_list (list): List of NetworkX graphs.
        n_samples (Int): Parameters for graphlet_sampling

    Returns:
        np.ndarray: Kernel matrix K
    """
    graphs = []

    for graph in tqdm(graph_list):
        graph_grakel = Graph(nx.adjacency_matrix(graph))
        graphs.append(graph_grakel)

    gk = GraphKernel(kernel=[
        {"name": "graphlet_sampling", "sampling": {"n_samples": n_samples}},
        {"name": "shortest_path"},
        {"name": "random_walk"}
    ], normalize=True)

    K = gk.fit_transform(graphs)

    return K


def get_neighbouring_cities(target_city, given_cities, K, num_neighbour=5):
    """
    Returns the neighbouring cities based on the similarity matrix.

    Args:
        target_city (str): The name of the city.
        given_cities (list): List of all cities.
        K (np.ndarray): The similarity matrix.
        num_neighbour (int): The number of neighbouring cities to return. Default is 7.

    Returns:
        neighbouring_cities (list): List of neighbouring cities.
    """

    if target_city in given_cities:
        city_idx = given_cities.index(target_city)

        sorted_indices = np.argsort(K[city_idx])

        assert target_city == given_cities[sorted_indices[-1]], "the most similar city must be the city itself"

        highest_value_indices = sorted_indices[::-1][:1 + num_neighbour]
        neighbouring_cities = [given_cities[idx] for idx in highest_value_indices]

        return neighbouring_cities
    else:
        raise ValueError("City not found in list of all cities")


def cluster_cities(K, cities, algorithm='kmeans', n_clusters=5, random_state=42):
    """
    Cluster cities using specified clustering algorithm.

    Args:
        K (np.ndarray): Similarity matrix.
        cities (list): List of city names.
        algorithm (str, optional): Clustering algorithm to use. Options are 'spectral', 'affinity', 'kmeans'. Defaults to 'kmeans'.
        n_clusters (int, optional): Number of clusters to form for 'spectral' and 'kmeans' clustering. Defaults to 5.
        random_state (int, optional): Seed used by random number generator for 'kmeans' clustering. Defaults to 42.

    Raises:
        ValueError: If an invalid algorithm name is provided.

    Returns:
        dict: A dictionary where the keys are the cluster numbers and the values are lists of city names in each cluster.
    """
    if algorithm == 'spectral':
        clustering = SpectralClustering(n_clusters=n_clusters, affinity='precomputed')
    elif algorithm == 'affinity':
        clustering = AffinityPropagation()
    elif algorithm == 'kmeans':
        clustering = KMeans(n_clusters=n_clusters, random_state=random_state)
    else:
        raise ValueError("Invalid algorithm name. Choose from 'spectral', 'affinity', 'kmeans'.")

    labels = clustering.fit_predict(K)

    if algorithm == 'affinity':
        n_clusters = len(clustering.cluster_centers_indices_)

    clusters = {}
    for i in range(n_clusters):
        clusters[i] = list(np.array(cities)[labels == i])

    return clusters


# ================================================
# ============== Tuning Model part ===============
# ================================================


def tuning_model(lr: float, weight_decay: float, hidden_dim: int, device: str, model_type, num_features, train_split,
                 valid_split, criterion, regression):
    """
    Trains a PyTorch model with given hyperparameters and evaluates it.

    The model is trained on the training set and evaluated on the validation set.
    The evaluation metric is the root mean squared error (RMSE) for regression tasks and
    F1 score for classification tasks.

    Parameters:
    lr: float - The learning rate.
    weight_decay: float - The weight decay.
    hidden_dim: int - The number of hidden units in the model.
    device: str - The device to train the model on.
    model_type: torch.nn.Module - The type of the model to train.
    num_features: int - The number of features in the input data.
    train_split: Tuple[Graph, Tensor] - A tuple containing the training graph and targets.
    valid_split: Tuple[Graph, Tensor] - A tuple containing the validation graph and targets.
    criterion: torch.nn.Module - The loss function.
    regression: bool - Whether the task is a regression task or classification task. If True,
                       the task is a regression task. Otherwise, it's a classification task.

    Returns:
    metric: float - The evaluation metric. Negative RMSE for regression tasks and F1 score for classification tasks.
    """
    hidden_dim = int(hidden_dim)  # cast to int because BayesianOptimization explores the space with floats

    # Define model
    model = model_type(num_features, 1, hidden_dim=hidden_dim)
    model = model.to(device)

    # Define optimizer with hyperparameters to optimize
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

    result_dict = pyg_train(
        model=model,
        device=device,
        train_split=train_split,
        valid_split=valid_split,
        batch_size=1,
        optimizer=optimizer,
        criterion=criterion,
        verbose=False,
        regression=regression
    )

    if regression:
        # negate the MSE to make lower MSE better
        mse = result_dict["eval_metrics"]['mse'][-1]
        rmse = np.sqrt(mse)
        return -rmse
    else:
        f1 = result_dict["eval_metrics"]['f1-score'][-1]
        return f1


def plot_eval(metrics_dict, regression=True):
    """
    Plot evaluation metrics for regression or classification.

    Args:
        metrics_dict (dict): Dictionary containing the metrics to plot.
        regression (bool): Whether the task is regression (True) or classification (False).
    """
    # If regression, plot MSE and MAE.
    if regression:
        plt.plot(metrics_dict['x'], metrics_dict['mse'], label='MSE')
        plt.plot(metrics_dict['x'], metrics_dict['mae'], label='MAE')
    # If classification, plot Accuracy and F1-Score.
    else:
        plt.plot(metrics_dict['x'], metrics_dict['accuracy'], label='Accuracy')
        plt.plot(metrics_dict['x'], metrics_dict['f1-score'], label='F1-Score')

    plt.xlabel('Iterations')
    plt.ylabel('Value')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.show()


def plot_loss(loss, cities, target_city):
    """
    Plot loss for each neighboring city to the target city.

    Args:
        loss (list): List of loss values for each iteration.
        cities (list): List of all cities.
        target_city (str): Name of the target city.
    """
    neighbours = [city for city in cities if city != target_city]
    num_neighbour = len(neighbours)
    for i in range(num_neighbour):
        plt.plot(loss[i::num_neighbour], label=neighbours[i])
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')


def create_objective_pyg(device, model_type, num_features, train, valid, criterion, regression):
    """
    Create objective function for PyTorch Geometric model tuning.

    Args:
        device (str): Device to run the model on.
        model_type (str): Type of the PyTorch Geometric model.
        num_features (int): Number of features in the input data.
        train (torch_geometric.data.Data): Training dataset.
        valid (torch_geometric.data.Data): Validation dataset.
        criterion (torch.nn.Module): Loss function.
        regression (bool): Whether the task is regression (True) or classification (False).

    Returns:
        function: The objective function to be optimized.
    """

    def objective(trial):
        lr = trial.suggest_float('lr', 1e-5, 1e-3)
        weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3)
        hidden_dim = trial.suggest_int('hidden_dim', 512, 1028)
        return tuning_model(lr, weight_decay, hidden_dim, device, model_type, num_features, train, valid, criterion,
                            regression)

    return objective


def create_objective_sklearn(train_split, valid_split, aggr_operator, regression=True):
    """
    Creates an objective function for Optuna hyperparameter tuning.

    The objective function trains a model with given hyperparameters and evaluates
    it on the validation set. The evaluation metric is Mean Squared Error (MSE) for regression
    tasks and F1 score for classification tasks.

    Parameters:
    train_split: Tuple[Graph, Tensor] - A tuple containing the training graph and targets.
    valid_split: Tuple[Graph, Tensor] - A tuple containing the validation graph and targets.
    aggr_operator: str - The aggregation operator used for aggregating feature vectors in GraphSAGE.
    regression: bool - Whether the task is a regression task or classification task. If True,
                       the task is a regression task. Otherwise, it's a classification task.

    Returns:
    objective: Callable - The objective function for Optuna.
    """

    def objective(trial):
        # Suggested hyperparameters for the model.
        params = {
            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1),
            'max_depth': trial.suggest_int('max_depth', 1, 9),
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'subsample': trial.suggest_uniform('subsample', 0.1, 1),
            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1),
            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1.0),
            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1.0),
            'random_state': 42
        }

        # For regression tasks, use XGBRegressor and return the MSE.
        if regression:
            model = xgb.XGBRegressor(**params)
            results = sklearn_train(
                model,
                train_split,
                valid_split,
                edge_aggr=aggr_operator,
                verbose=False
            )
            return results["valid_metrics"]["mse"]

        # For classification tasks, use XGBClassifier and return the F1 score.
        else:
            model = xgb.XGBClassifier(**params)
            results = sklearn_train(
                model,
                train_split,
                valid_split,
                edge_aggr="sum",
                normalize_targets=False,  # Very important for link prediction
                train_criterions=["accuracy", "f1"],
                valid_criterions=["accuracy", "f1"],
            )
            return results["valid_metrics"]["f1"]

    return objective
